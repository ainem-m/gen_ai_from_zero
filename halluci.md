LLM（大規模言語モデル）の**ハルシネーション（hallucination、幻覚）**とは、一見もっともらしく見えながら実際には正しくない文を生成する現象を指します [1]。この現象はLLMを活用する上で大きな課題となっており、以下のような特徴を持ちます [1]:
*   文法的に正しく、言語として大きな不自然さがない [1]。
*   内容が事実に反していたり、論理的な矛盾がある [1]。
*   信じ込んでしまいそうなくらい説得力がある [1]。

ハルシネーションはLLMの応用において非常に厄介な問題であり、様々な対策が研究されています [1]。

### ハルシネーションの分類

ハルシネーションは、主に以下の観点から分類されます。

1.  **事実性ハルシネーション (Factual Hallucination)** [2]
    *   **定義**: LLMが生成した文が**現実世界の事実と一致しない**場合に発生するハルシネーションです [2]。現実世界における事実関係と整合性がとれていない回答を指します [2]。
    *   **類型**:
        *   **事実との矛盾 (factual inconsistency)**: 現実世界の事実と矛盾することが検証可能なケース [3]。
            *   例: 「神奈川県の県庁所在地は、かながわ市です」という回答は、現実世界の事実（横浜市）と矛盾します [3]。
        *   **事実の偽造 (factual fabrication)**: 現実世界の事実に基づかないケースや、検証不能なケース [3]。
            *   例: 存在しない論文のURLを生成するなど、現実世界に根拠を見出せない「作り話」を生成するケースが該当します [3, 4]。

2.  **忠実性ハルシネーション (Faithfulness Hallucination)** [5]
    *   **定義**: LLMが生成した文が、**与えられたプロンプトや文脈と一致しない**場合に発生するハルシネーションです [2, 5]。内容が事実であるかどうかは別として、入力文の文脈や質問の意図と整合性がとれていない回答を指します [5]。
    *   **類型**:
        *   **指示不一致型 (instruction inconsistency)**: ユーザーの指示の意図とは異なる回答をするケース [5]。
        *   **文脈不一致型 (context inconsistency)**: ユーザーが提示している情報とLLMの回答が矛盾しているケース [5]。
        *   **論理不一致型 (logical inconsistency)**: LLMが生成した文が、論理的に自己矛盾しているケース [5]。
        *   **消極型 (ambiguity, under-informativeness)**: 明言を避けた曖昧な回答や、"I don't know"など無知を装った回答、LLMの悪用防止のための定型句を濫用するケース [5]。
    *   **ユーザーの指示に従い、指示不一致型や回答拒否（消極型の一部）は「幻覚」ではなくタスク失敗として扱います。** これらのケースは、LLMが質問の意図を理解できなかったり、回答を不必要に差し控えたりする挙動であり、ハルシネーション（事実と異なる内容を生成する現象）とは区別されるべきです [5]。
    *   **事実性との優先順位**: AIアバターとの会話目的であれば忠実性が重要ですが、仕事や勉強のサポート目的であれば事実性がより重要となります [6]。ただし、過去の誤った情報への過度な忠実性は、ハルシネーションが雪だるま式に増幅するリスクも指摘されています [6]。

3.  **内在型ハルシネーション (Intrinsic Hallucination)** [6]
    *   **定義**: 入力文に書かれた情報と、LLMによる生成文の間に**矛盾が存在する**ハルシネーションです [6]。
    *   例: 24時間営業の商業施設が多いという記述と、深夜営業が禁止されているという記述が同じ回答内に存在し、自己矛盾しているケースなどが挙げられます [7]。

4.  **外在型ハルシネーション (Extrinsic Hallucination)** [6]
    *   **定義**: LLMの生成文の根拠が、**入力文の中に存在しない**ハルシネーションです [6]。つまり、元の文脈から論理的に確定できない新情報をLLMが勝手に追加するケースを指します [8]。
    *   例: 「Xさんは高校時代にバンドでギターを弾いていました。Xさんはダンスもできますか？」という質問に対し、「Xさんはダンスも得意です」と回答するケース。質問文にダンスのスキルに関する情報がないにもかかわらず、LLMが断定してしまう点が該当します [7, 8]。
    *   外在型ハルシネーションであっても、LLMの内部知識に基づいて事実と合致している場合もあります [8]。この場合、ユーザが見落としていた情報をLLMが適切に補ってくれたと肯定的に捉えるか、勝手に情報を追加したと問題視するかは、LLMの回答利用用途によって判断が必要です [8]。

### ハルシネーションが発生する主な原因

ハルシネーションの発生原因は、主に以下の3つに分類されます [9]。

*   **学習データに由来するハルシネーション** [9]:
    *   **学習データの誤情報やバイアス**: 学習データに含まれる誤った記述や俗説をLLMが模倣してしまうことがあります [10]。大規模なLLMほど記憶容量が大きく、このリスクが高い傾向があります [10]。また、LLMの学習日以降の最新情報が不足していることも原因となります [11]。
    *   **ロングテールの裾野に位置する知識の不足**: 学習データがジップの法則などに従い、稀な知識（ロングテールに位置する知識）は学習が不十分となり、ハルシネーションの原因になります [11, 12]。
*   **学習に由来するハルシネーション** [12]:
    *   **露出バイアス**: 自己回帰モデルの構造的な課題により、生成が長くなるにつれて誤差が蓄積され、ハルシネーションにつながる場合があります [12]。
    *   **RLHF（人間からのフィードバックによる強化学習）によって生じるハルシネーション**: LLMが人間の評価者に過剰に迎合し、「事実ではないが人間は好みやすい」回答（忖度）を生成するリスクがあります [13]。
    *   **ショートカット学習**: LLMが本質的な理解ではなく、キーワードの共起や文体など、非本質的なパターンを覚えてしまうことで、誤った推論を導き出すことがあります [13, 14]。
*   **推論時に生じるハルシネーション** [14]:
    *   **逆転の呪い**: 「AはBだ」を学習しても「BはAだ」という文の生成に失敗する現象が報告されています [14]。
    *   **系列位置効果**: プロンプトの中間に書かれた情報をLLMが活用しにくい傾向があることが指摘されています [15]。
    *   **知識の競合**: LLMの内部パラメータに埋め込まれた知識が、検索結果などの外部知識より優先され、忠実性ハルシネーションを引き起こすことがあります [15]。

### ハルシネーション抑制のための対策

ハルシネーションを抑制するための対策は多岐にわたります [16]。

1.  **学習データの改善** [16]
    *   **フィルタリング**: Webクローリングで収集したデータから、目的外の言語やドメイン外のデータ、不適切なコンテンツを除去します [17-21]。重複データの除去も行われます [18-20]。
    *   **合成データの利用**: LLMや既存のソフトウェアを用いて高品質なテキストデータを自動生成し、学習に使用します [22]。これにより、特定のバイアス（忖度など）の抑制も可能です [22]。
    *   **指示チューニングの改善**: LLMの性能は指示チューニングの品質に大きく影響されるため、少量の高品質なデータを使用することが重要です [23, 24]。
    *   **RLHFによるアラインメント**: 人間からのフィードバックを報酬としてLLMをチューニングし、人間にとって好ましい、より人間的な感覚に近づけた回答を生成するように調整します [25, 26]。報酬モデリングと強化学習（PPO、DPOなど）が用いられます [25-34]。

2.  **デコーディング方法の改善** [34]
    *   **ペナルティの導入**: すでに出力した単語と似た単語の繰り返し出力を抑制するため、類似度が高い単語にペナルティを課す方法が提案されています [35]。また、小規模な「アマチュア言語モデル」の振る舞いの違いを活用して、誤った単語の選択を抑制する方法もあります [35, 36]。
    *   **LLMの内部状態にもとづくデコーディング制御**: Transformerの浅い層（統語情報）と深い層（意味論・知識情報）の出力の差を利用することで、事実性を強調した生成が可能となります（DoLaなど）[36, 37]。

3.  **モデル構造の改良** (これらの技術は、直接のハルシネーション対策というよりも、LLMの基礎的な性能、特に長文処理能力を向上させるための改良です。)
    *   **自己アテンションの改良**: 長文処理のボトルネックとなる自己アテンションの計算量を削減するための手法です [38]。
        *   **フラッシュアテンション**: GPU上のデータ転送を最適化し、計算を高速化します [39, 40]。
        *   **スパースアテンション**: すべてのトークンペア間の計算を避け、一部のみに限定することで計算量を削減します（Reformer、BigBirdなど）[38]。
        *   **線形アテンション**: カーネル関数を用いて計算量を線形オーダに削減します（Performerなど）[41-44]。
    *   **位置符号の改良**: 長文を効率的に扱えるよう、位置情報の注入方法を工夫します [45, 46]。
        *   **ALiBi**: 線形バイアスで相対位置関係を表現します [47]。
        *   **RoPE (Rotary Positional Embedding)**: 回転行列を用いて位置情報を埋め込みます [48]。
        *   **位置内挿 (Positional Interpolation, PI)**: 学習済みのLLMで扱える文脈長を、位置符号の範囲を引き伸ばすことで拡大します [47, 49]。
    *   **状態空間モデル (State Space Model, SSM)**: Transformerの欠点を克服し、長文を効率的に処理するための新しいアーキテクチャ（RetNet、RWKV、Mambaなど）が注目されています [47, 50-53]。

4.  **プロンプトエンジニアリング (Prompt Engineering)** [52]
    *   LLMの内部に手を触れずに、入力（プロンプト）の工夫によってLLMの挙動を制御する手法です [52, 54]。
    *   **思考の連鎖 (Chain-of-Thought, CoT)**: LLMに思考過程を段階的に出力させることで、より正確な推論を促し、ハルシネーションを抑制します [54]。
    *   **自己整合性プロンプト (Self-consistency Prompt)**: 複数の思考経路を生成させ、最も一貫性のある回答を選択します [55]。
    *   **出力の検証 (Verification)**: LLMが生成した情報の誤りを検証器（Verifier）によって確認し、品質を改善します [55]。
        *   **CoVe (Chain-of-Verification)**: 生成された回答内の各主張に対し、検証用の質問を生成して正確性を確認します [56, 57]。
        *   **自己検証 (Self-verification)**: LLMに自身の推論ステップの妥当性を検証させることで、途中で生じるハルシネーションの連鎖を防ぎます [57, 58]。
    *   **プロンプトの自動生成**:
        *   **方向性刺激プロンプティング (Directional Stimulus Prompting, DSP)**: 補助的なLLMでヒントを生成し、メインのLLMの回答を誘導します [59]。
        *   **マルチエージェント型LLM**: 複数のLLMエージェントに異なる役割を与えて対話させることで、協力的に問題を解決し、最終的な出力の品質を向上させます [60, 61]。
        *   **LLMフレームワーク**: LangChainやLlamaIndexなどのフレームワークを活用し、複雑なプロンプトパイプラインを効率的に開発します [62]。

5.  **外部知識活用にもとづく生成** [63]
    *   LLMの内部にない知識を外部から取得し、それを活用することでハルシネーションを抑制する手法です [63]。
    *   **検索拡張生成 (Retrieval-Augmented Generation, RAG)**: 検索エンジンを利用して、必要な情報が記述された文書を検索し、その検索結果をLLMへのプロンプトに組み込むことで、事実性の高い回答を生成させます [63, 64]。
        *   検索方法には、文字列の類似性に基づく**疎ベクトル検索**（TF-IDF、BM25など） [65-69] と、意味論的な類似性に基づく**密ベクトル検索**（DPR、文埋め込みなど）[69-74] があります。効率化のために近似最近傍探索（HNSW、IVF-PQなど）が利用されます [42, 43, 75, 76]。
        *   検索結果の質を高めるために、**クエリリライタ** [76, 77]や**リランカ** [78]を組み合わせることもあります。また、疎ベクトル検索と密ベクトル検索を組み合わせた**ハイブリッド検索**も有効です [79]。
    *   **ツール拡張生成 (Tool-Augmented Generation)**: LLMが外部の専門的なツール（SQL、Pythonインタプリタ、記号処理システム、物理シミュレータなど）を利用して、より正確な情報を取得したり、複雑な推論を行ったりする手法です [55, 58, 63, 80]。
        *   **Text-to-SQL**: 自然言語からSQLクエリを生成し、RDBMSから情報を取得します [55, 56]。
        *   **コードインタプリタ**: LLMがプログラム（Pythonなど）を生成・実行し、複雑な計算問題などを正確に解きます（ChatGPTのCode Interpreterなど）[81-85]。
        *   **記号処理システム**: 数学的な計算機代数システム（Wolfram Alpha、SymPyなど）や定理証明支援システム（Coqなど）を活用し、厳密な論理的推論や計算を行います [63, 65, 66, 85-91]。
        *   **シミュレータ**: 物理現象などの実世界の挙動をシミュレーションすることで、LLMが科学的に正確な記述をできるようにします（DeepMindのMind's Eyeなど）[92-94]。
