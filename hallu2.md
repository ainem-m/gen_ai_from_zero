ハルシネーションのタイプ

ハルシネーションは主に次の2つのタイプに分けられます。

①事実性ハルシネーション（Factual Hallucination）

実際の世界の事実とは異なる情報を生成するケースです。
	•	事実との矛盾：例）「神奈川県の県庁所在地はかながわ市です」（正しくは横浜市）
	•	事実の偽造：例）「存在しない論文のURLを提示する」

②忠実性ハルシネーション（Faithfulness Hallucination）

質問や指示の内容に忠実でない回答を生成するケースです。
	•	指示不一致：例）「リンゴについて教えて」と指示したのに、ミカンの情報を返す
	•	文脈不一致：例）質問に含まれる情報を無視して回答する
	•	論理的不一致：例）回答の中で論理が破綻している
	•	消極的回答：例）曖昧に答えたり、「分かりません」を連発して逃げる

ただし、消極的回答は「幻覚」ではなく、タスクの失敗として扱うこともあります。

ハルシネーションの原因はどこにある？

ハルシネーションが起こる理由は大きく分けて以下の3つがあります。

①学習データに原因がある場合
	•	誤情報や古い情報を学習してしまう：学習データが古かったり、誤った情報を含んでいる場合に、そのまま生成されることがあります。
	•	レアな知識が不足している：あまり知られていない知識は学習が不十分なため、間違いやすくなります。

②学習方法に原因がある場合
	•	露出バイアス：文章生成が長くなるほど、わずかな誤差が蓄積され、結果的に誤情報を作り出すことがあります。
	•	人間のフィードバック（RLHF）に迎合する：AIが人間の好みに合わせて、事実ではないが好まれそうな回答を出すことがあります。
	•	ショートカット学習：本質的な理解ではなく、表面的なパターンを覚えてしまうことで、誤った推論をしてしまいます。

③推論プロセスに原因がある場合
	•	逆転の呪い：「AはBだ」と学習しても、「BはAだ」と逆方向には推論できないことがあります。
	•	系列位置効果：プロンプトの途中に書いた情報は、うまく使われないことがあります。
	•	知識の競合：内部の知識が検索結果よりも優先され、間違いを起こす場合があります。

ハルシネーションを抑えるために

ハルシネーションへの対策は日々進んでいます。その代表的なものは以下の通りです。

①学習データの改善
	•	不適切なデータの除去（フィルタリング）
	•	信頼できる情報だけを使った「合成データ」での学習

②検索機能の活用
	•	ChatGPTのようなAIには検索機能が組み込まれ、出典を提示できるモデルがあります（例：ChatGPT-4oのBrowse機能）。これにより、回答の信憑性をすぐに確認できます。

③RAG（Retrieval-Augmented Generation）の活用
	•	自分が信頼している資料（論文やガイドライン）をAIに与えて回答を生成させる方法です。AIが自分の手持ち情報だけから回答するため、誤情報のリスクを大きく下げることができます。

これらの対策を適切に組み合わせれば、AIの「ウソ」をかなり減らせる可能性があります。
