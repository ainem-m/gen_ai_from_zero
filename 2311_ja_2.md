# 大規模言語モデルにおけるハルシネーションに関する調査：原理・分類体系・課題・未解決問題

黄磊*, ハルビン工業大学, 中国  
余威江, 華為技術有限公司, 中国  
馬偉涛・鍾偉宏, ハルビン工業大学, 中国  
馮正尹・王豪天, ハルビン工業大学, 中国  
陳強龍・彭ウェイ華, 華為技術有限公司, 中国  
馮暁成*, 秦彬・劉婧婧, ハルビン工業大学, 中国

*corresponding author

---

## 概要
大規模言語モデル（LLM）の登場は自然言語処理分野における画期的な進展であり、情報取得パラダイムに根本的な変革をもたらしました。しかしながら、LLMは幻覚現象を起こしやすいという特性があり、これは一見妥当に見えるが実際には事実に基づかない内容を生成する現象です。この現象は、LLMが現実世界の情報検索（IR）システムにおいてどの程度信頼できるかという重大な懸念を引き起こしており、このような幻覚現象を検出・軽減するための研究が精力的に行われています。

LLMが持つオープンエンドで汎用的な特性を考慮すると、LLMの幻覚現象は従来のタスク特化型モデルとは異なる特有の課題を提起しています。この相違は、LLMの幻覚現象に関する最新の研究成果を精緻に理解し、包括的に把握することの緊急性を浮き彫りにしています。

本調査では、まずLLM時代における幻覚現象の革新的な分類体系を提示し、続いて幻覚現象を引き起こす要因について詳細に考察します。その後、幻覚現象の検出手法とベンチマークに関する包括的な概要を提示します。議論はさらに、LLMの幻覚現象を軽減するための代表的な方法論へと展開します。加えて、検索拡張型LLMが幻覚現象に対処する際に直面する現在の限界についても考察し、より堅牢な情報検索システム構築に向けた知見を提供します。最後に、LLMの幻覚現象に関する有望な研究方向性について言及します。これには、大規模視覚言語モデルにおける幻覚現象や、LLMの幻覚現象における知識境界の理解といった研究分野が含まれます。

---

# 1. はじめに

近年、大規模言語モデル（LLM）[383]、特にLLaMA[299, 300]、Claude[9]、Gemini[7, 259]、GPT-4[232]などの登場により、自然言語処理（NLP）分野において画期的なパラダイムシフトが生じ、言語理解[116, 124]、生成[373, 393]、推論[57, 151, 250, 326, 354]の分野で前例のない進歩が達成されました。さらに、LLMに組み込まれた広範な事実知識は、情報検索[6, 246]におけるLLMの活用において顕著な進歩をもたらしており、情報検索システム[394]の設計思想に変革をもたらす可能性を秘めています。

しかしながら、これらの目覚ましい進歩と並行して、LLMが幻覚を生成する傾向[15, 105]に対する懸念も生じており、一見妥当に見えるものの実際には事実に基づかない内容が生成される事例が報告されています。さらに問題を複雑化させているのは、LLMが非常に自然で人間らしい応答を生成する能力[265]であり、これが幻覚の検出を特に困難にしています。このような状況下では、LLMの実用的な展開、特にチャットボット[8, 231]、検索エンジン[4, 214]、推薦システム[97, 171]など、日常生活に浸透したシステムへの統合において、幻覚の問題が特に重要な課題となっています。これらのシステムが提供する情報は意思決定に直接影響を与える可能性があるため、誤った情報は偽の信念を拡散させたり、場合によっては害を及ぼす可能性さえあります。

従来の自然言語生成（NLG）タスクにおける幻覚現象については、すでに広範な研究が行われています[125, 136]。幻覚とは、生成された内容が意味をなさない、あるいは与えられた情報源の内容と整合しない生成結果を指します。これらの幻覚は2つのタイプに分類されます：内在的幻覚では、生成結果が情報源と矛盾し、外在的幻覚では、生成結果が情報源から検証不能です。ただし、これらの現象はタスク横断的な汎用性において顕著な特徴を示しており[15, 30]、特定タスク向けに設計されたモデルと比較して、LLMにおける幻覚現象の理解は独自の課題を提起します。さらに、LLMは通常オープンエンドなシステムとして機能するため、幻覚の範囲はより広範な概念を包含し、主に事実誤認として顕在化します。この変化は、既存の幻覚分類体系を再評価・調整し、進化するLLMの状況においてその適応性を高める必要性を示唆しています。

本研究では、大規模言語モデル（LLM）における幻覚現象に特化した新たな分類体系を提案します。幻覚現象を「事実性幻覚」と「忠実性幻覚」の2つの主要タイプに分類します。事実性幻覚は、生成されたコンテンツと検証可能な現実世界の事実との間に生じる不一致を強調するもので、典型的には事実的な不整合として現れます。一方、忠実性幻覚は、生成コンテンツがユーザー入力から逸脱する場合や、生成コンテンツ自体の自己整合性が欠如している場合を捉えています。このカテゴリーはさらに細分化され、指示内容の不整合、提供された文脈からの不一致、コンテンツ内の内部矛盾といった具体的な形態に分類されます。このような分類体系は、LLMにおける幻覚現象の理解を深め、現代の使用状況と密接に整合させるものです。

さらに、LLMにおける幻覚現象の根本的な原因を探求することは、単にこれらの現象の理解を深めるためだけでなく、それらを軽減するための戦略を策定する上でも不可欠です。LLMにおける幻覚現象の多面的な原因を認識した上で、本調査ではその原因をデータ処理段階、学習段階、推論段階という3つの主要な側面に分類しています。この分類体系により、幅広い要因を網羅的に検討することが可能となり、LLMシステムにおける幻覚現象の発生源とそのメカニズムについて包括的な理解が得られます。

さらに本調査では、LLMにおける幻覚現象を検出するために特別に考案された多様な効果的な検出手法を包括的に概説するとともに、LLMにおける幻覚現象に関連するベンチマークについて網羅的に概説しています。これらは、LLMが生成する幻覚現象の範囲を評価し、検出手法の有効性を検証するための適切なテストベッドとして機能します。評価を超えて、LLMにおける幻覚現象を軽減するための重要な取り組みが実施されています。本研究では、これらの取り組みを包括的に調査し、データ関連、学習プロセス関連、推論関連といった各要因に沿って体系的に分析しています。

さらに、生成モデルにおける検索拡張生成（RAG）手法が幻覚現象の軽減に果たす有効性については、この分野において多大な注目を集めています。RAG手法が持つ大きな可能性にもかかわらず、現行システムには本質的な限界が存在し、場合によっては幻覚現象を引き起こすこともあります。このような背景から、本調査ではこれらの課題について詳細に分析し、より堅牢なRAGシステムの開発に向けた有益な知見を提供することを目的としています。さらに、大規模視覚言語モデルにおける幻覚現象や、LLMの幻覚現象における知識の限界の理解など、今後の研究に向けた有望な方向性についても言及しており、この分野における今後の研究の道筋を示しています。

**既存研究との比較:**

生成AIにおける幻覚現象が主要な研究課題として浮上して以来、数多くの研究[136, 192, 258, 298, 312, 376]がこの現象の解明に向けて取り組まれてきました。これらの研究は様々な視点からLLMの幻覚現象を探求し、貴重な知見をもたらしてきましたが、本調査では特にこれらの研究の独自の貢献範囲と包括的な研究対象範囲を明らかにすることを目的としています。

- **Jiら[136]**: 主に自然言語生成タスク向けの事前学習モデルにおける幻覚現象に焦点を当てており、LLMはこの研究範囲に含まれていません。
- **Tonmoyら[298]**: 主にLLMの幻覚現象に対処するための緩和戦略について論じています。
- **Liuら[192]**: LLMの信頼性に関するより広範な視点を提示していますが、特定の幻覚現象の詳細には踏み込んでいません。
- **Wangら[312]**: LLMにおける事実性について詳細な分析を行っています。

しかしながら、本調査では特に信頼性に関する課題の中でも幻覚現象に焦点を当て、さらに忠実性に関する幻覚現象についても考察範囲を広げています。現時点で我々が知る限りでは、Zhangら[376]は本調査と密接に関連する研究を発表しており、LLMにおける幻覚現象の分類体系、評価ベンチマーク、および緩和戦略について詳細に論じています。しかしながら、本調査では独自の分類体系と構成構造によって他との差別化を図っています。我々は幻覚現象について詳細な層別分類を提示するとともに、幻覚現象の原因についてより包括的な分析を行っています。特に、我々が提案する緩和戦略はこれらの原因に直接結びついており、LLMにおける幻覚現象に対処するための体系的かつ整合性のある枠組みを提供しています。

**本調査の構成:**

本調査では、LLMにおける幻覚現象に関する最新の進展状況を包括的に概説します（図1参照）。
1. LLM領域における幻覚現象の分類体系を構築します（セクション2）。
2. LLMにおける幻覚現象の原因について詳細に分析します（セクション3）。
3. LLMにおける幻覚現象を確実に検出するために用いられる様々な戦略とベンチマークについて概説します（セクション4）。
4. これらの幻覚現象を緩和するために設計された様々なアプローチについて詳細に述べます（セクション5）。
5. 現在のRAGシステムが直面している課題について掘り下げます（セクション6）。
6. 今後の研究に向けた潜在的な方向性について概説します（セクション7）。

![図1. 本調査の主要な内容フローと分類体系](img-0.jpeg)

# 2. 定義

LLMにおける幻覚現象を包括的に理解するため、まずLLMの概要（2.1）について簡潔に説明し、この調査の対象範囲を明確にします。続いて、LLMの学習段階について詳述します（2.2）。学習メカニズムを十分に理解することは、幻覚現象の発生原因を解明する上で極めて重要です。最後に、LLMにおける幻覚現象の概念についてさらに詳しく説明し（2.3）、これを2つの異なるタイプに分類します。

## 2.1. 大規模言語モデル

幻覚現象の原因を探る前に、まず大規模言語モデル（LLM）の概念について説明します。一般的にLLMとは、トランスフォーマーベースの言語モデルアーキテクチャを採用し、膨大な量のテキストコーパスを用いて徹底的な学習を行った汎用モデルの総称です。代表的な例として、GPT-3 [29]、PaLM [54]、LLaMA [300]、GPT-4 [232]、Gemini [259]などが挙げられます。データ量とモデルの規模を拡大することで、LLMは驚くべき創発的な能力を獲得し、具体的には文脈学習（ICL）[29]、思考連鎖プロンプティング[326]、指示実行能力[244]などが典型的な例として知られています。

## 2.2. 大規模言語モデルの学習フェーズ

大規模言語モデル（LLM）の特性と振る舞いは、その訓練プロセスと深く密接に関連しています。LLMには主に3つの訓練段階があります：事前学習、教師あり微調整（SFT）、人間のフィードバックによる強化学習（RLHF）です。これらの段階を分析することで、LLMにおける幻覚現象の発生メカニズムを理解する手がかりが得られます。各訓練段階はそれぞれ異なる能力をモデルに付与するためです。

### 2.2.1. 事前学習

事前学習はLLMが知識と能力を獲得する基盤段階として広く認識されています[388]。この段階において、LLMは系列内の後続トークンを自己回帰的に予測します。大規模なテキストコーパスを用いた自己教師あり学習を通じて、LLMは言語の構文構造、世界知識、および推論能力を獲得し、さらなる微調整のための強固な基盤を構築します。さらに、最近の研究[72, 291]では、後続トークンの予測は重要な情報を損失なく圧縮するプロセスに類似していると示唆されています。大規模言語モデル（LLM）の真価は、後続する単語の出現確率分布を予測する能力にあります。この予測精度の高さは、モデルが膨大な知識を体系的に把握していることを意味し、結果として世界に対する深い理解力を備えていると言えます。

### 2.2.2. 教師あり微調整

LLMは事前学習段階で多大な知識と能力を獲得しますが、事前学習では主に文章補完を最適化するように学習されています。このため、事前学習済みLLMは基本的に文章補完モデルとして機能し、これがLLMの次単語予測という目的とユーザーの求める適切な応答という目的との間にずれを生じさせる要因となります。このずれを解消するために、教師あり微調整（SFT）[370]が導入されました。これは注意深くアノテーションされた（指示文-応答文）ペアセットを用いてLLMをさらに学習させる手法であり、これによりLLMの能力が向上するとともに、制御可能性も改善されます。さらに最近の研究[60, 129]では、教師あり微調整が未知のタスクにおいても優れた性能を達成できることが確認されており、これはLLMが顕著な汎化能力を備えていることを示しています。

### 2.2.3. 人間フィードバックからの強化学習

SFTプロセスによってLLMがユーザー指示に従えるようになる一方で、人間の嗜好により良く合致させるための改善の余地が残されています。人間フィードバックを活用する様々な手法の中でも、強化学習からの人間フィードバック（RLHF）は、強化学習を通じて人間の嗜好に合致させるための代表的な解決策として登場しています[55, 233, 285]。通常、RLHFでは、プロンプトと人間によってラベル付けされた応答ペアを与えられた際に選好順位を予測するように訓練された選好モデル[26]が使用されます。人間の選好に合致させるために、RLHFではLLMが訓練された選好モデルによって提供される報酬を最大化するような出力を生成するように最適化されます。これは通常、Proximal Policy Optimization（PPO）[270]などの強化学習アルゴリズムを用いて実装されます。このような人間フィードバックを訓練ループに統合する手法は、LLMの人間の嗜好への合致性を向上させる上で効果的であり、高品質で無害な応答を生成するようLLMを導く役割を果たします。

## 2.3. 大規模言語モデルにおける幻覚現象

幻覚という概念は、病理学と心理学の分野にその起源を持ち、実際には存在しない実体や事象を知覚する現象と定義されます[202]。自然言語生成の分野では、幻覚は通常、生成された内容が提供された情報源の内容と矛盾したり、信頼性に欠ける現象を指します[89, 208]。この概念は、人間の心理学において観察される幻覚現象と緩やかに類似しています。

自然言語生成タスクにおける幻覚は、主に2つのタイプに分類されます：**内在的幻覚**と**外在的幻覚**[126, 136, 174]です。
- **内在的幻覚**: モデルの出力が提供された情報源の文脈と直接矛盾する現象を指します。
- **外在的幻覚**: 提供された情報源の文脈や外部知識ベースを用いて検証できない出力を指します。つまり、生成されたテキストは利用可能な情報によって裏付けられておらず、直接矛盾するものでもないため、その出力は検証不可能であり、潜在的に誤解を招く可能性があります。

しかしながら、LLMの登場により、これらのモデルが持つ汎用性の高さが様々な分野での広範な活用を促進しており、従来のタスク特化型分類体系の限界が浮き彫りになっています。LLMがユーザー中心のインタラクションを重視し、ユーザー指示への忠実性を最優先する特性に加え、幻覚現象が主に事実レベルにおいて発生するという特徴を踏まえ、我々はJiら[136]の基礎的な研究を基盤としたより詳細な分類体系を提案します。この精緻化された分類体系は、LLMの幻覚現象に関連する特有の複雑性を包括的に捉えることを目的としています。LLMの幻覚現象の定義をより直感的に理解してもらうため、事実レベルの幻覚現象と忠実性レベルの幻覚現象について、それぞれ具体例を表1に示します。

### 2.3.1. 事実レベルの幻覚現象

LLMの登場は、従来のタスク特化型ツールキットから、オープンドメインインタラクションに重点を置いたAIアシスタントへの大きな転換点となっています。この現象は主に、大規模言語モデルが保有する膨大な事実知識に起因しています。しかし既存の大規模言語モデルでは、現実世界の事実と矛盾する、あるいは検証不可能な出力を生成する傾向が時折見られます[168]。これは人工知能の信頼性に対する重大な課題を提起しています。このような事実性に関する幻覚現象は、主に以下の2つの主要なタイプに分類されます：

- **事実的矛盾**: 大規模言語モデルの出力に含まれる事実が、現実世界の情報に基づいて検証可能であるにもかかわらず、矛盾を含んでいる場合を指します。このタイプの幻覚現象は最も頻繁に発生し、その原因は多様で、大規模言語モデルが事実的知識をどのように取得・保存・表現するかといった様々な要因に起因します。矛盾の種類に応じて、さらに以下の2つのサブカテゴリーに分類できます。
    - **エンティティ誤り型ハルシネーション**: LLMが生成するテキスト中に誤ったエンティティ（実在する人物や事物）が含まれる現象を指します。表1に示すように、「電話機の発明者」について尋ねた場合、モデルは誤って「トーマス・エジソン」と回答しますが、実際の発明者は「アレクサンダー・グラハム・ベル」であることが事実です。
    - **関係誤り型ハルシネーション**: LLMが生成するテキスト中に、エンティティ間の関係性について誤った記述が含まれる現象を指します。表1に示すように、「電球の発明者」について尋ねた場合、モデルは誤って「トーマス・エジソン」と回答しますが、実際には彼は既存の設計を改良したものであり、電球そのものを発明したわけではありません。

- **事実の捏造**: LLMが生成する出力内容の中に、既存の現実世界の知識と照合して検証不可能な事実が含まれている場合を指します。これはさらに以下の2つのサブカテゴリーに分類できます。
    - **検証不能な幻覚**: 完全に存在しない、あるいは利用可能な情報源によって検証できない発言を指します。表1に示すように、「エッフェル塔の建設による主な環境影響」について尋ねた場合、モデルは「この建設によってパリタイガーが絶滅した」と誤って回答しています。パリタイガーは実在しない種であるため、この主張はいかなる歴史的・生物学的記録によっても裏付けることができません。
    - **過剰主張型幻覚**: 主観的なバイアスにより普遍的な妥当性を欠く主張が生じる現象を指します。表1に示すように、モデルは「エッフェル塔の建設が、世界的なグリーン建築運動の発端となった出来事として広く認識されている」と主張しています。これは過剰主張の典型例であり、この主張を裏付ける広範な合意や実質的な証拠は存在しません。

### 2.3.2. 忠実性レベルの幻覚現象

LLMは本質的にユーザーの指示に沿うように訓練されています。LLMの利用がユーザー中心型アプリケーションへと移行するにつれ、ユーザーが提供する指示や文脈情報との整合性を確保することがますます重要になっています。さらに、LLMの忠実性は、その生成コンテンツの論理的一貫性にも反映されます。この観点から、忠実性型幻覚には以下に示す3つのサブタイプが存在します：

- **指示内容の不一致**: LLMがユーザーの指示から逸脱した出力を生成する現象を指します。一部の逸脱は安全ガイドラインに沿ったものかもしれませんが、ここで問題となる不一致は、悪意のないユーザー指示との意図的な不整合を意味します。表1に示すように、ユーザーの実際の意図は翻訳作業でしたが、LLMは誤ってユーザー指示から逸脱し、質問応答タスクを実行してしまいました。
- **文脈不一致**: LLMの出力がユーザー提供の文脈情報と整合性を欠いている事例を指します。例えば表1に示すように、ユーザーはナイル川の水源が中央アフリカの大湖地域にあると言及しているにもかかわらず、LLMの応答はこの文脈と矛盾していました。
- **論理的不整合**: LLMの出力において内部的な論理的矛盾が生じる現象を指し、特に推論タスクにおいて顕著に観察されます。これは推論過程自体の間、あるいは推論過程と最終的な解答との間に不整合が生じる形で現れます。例えば表1に示すように、方程式の両辺を2で割るという推論過程自体は正しいものの、最終的な解答「x=4」は推論の連鎖と矛盾しており、結果として誤った解答に至っています。

**表1. LLMの幻覚現象の各カテゴリーにおける具体例**
*(赤枠は幻覚的な出力、青枠はユーザー指示や文脈情報を示します)*

| カテゴリ | タイプ | 対話例 | 解説 |
| :--- | :--- | :--- | :--- |
| **事実性幻覚** | 事実誤認 | **指示**: トーマス・エジソンが科学技術分野において果たした主な貢献は何ですか？<br>**回答**: トーマス・エジソンは<font color="red">実用的な電話機を発明し、白熱電球を開発しました</font>。 | 回答には事実誤認があります。実際には、エジソンは既存の電球設計を改良したものであり、アレクサンダー・グラハム・ベルが電話機を発明しました。 |
| | 事実の捏造 | **指示**: エッフェル塔の建設が環境に及ぼした主な影響は何でしょうか？<br>**回答**: 1889年に建設されたエッフェル塔は、<font color="red">地域の生態系において重要な役割を担っていた「パリタイガー」の絶滅を引き起こしました。さらに、これは世界的なグリーン建築運動のきっかけとなった出来事として広く認識されています</font>。 | 回答には誤りがあり、エッフェル塔の建設が「パリタイガー」の絶滅を引き起こし、世界的なグリーン建築運動のきっかけとなったと誤って記載されています。実際には「パリタイガー」という種は存在しなかったため、これは捏造された主張です。さらに、グリーン建築運動の起源をこの出来事に帰している点も誤りです。 |
| **忠実性幻覚** | 指示矛盾 | **指示**: <font color="blue">「フランスの首都はどこですか？」という英語の質問をスペイン語に翻訳してください</font><br>**回答**: <font color="red">フランスの首都はパリです。</font> | ユーザーは明確に「英語の質問をスペイン語に翻訳してください」と指示しています。しかしLLMは指示通りの翻訳を行わず、質問に直接回答しています。 |
| | 文脈不整合 | **取得文書**: <font color="blue">ナイル川はアフリカ北東部を北流する主要な河川です。伝統的に世界最長の川とみなされています。中央アフリカの大湖地域を源流とし、複数の国を流れた後、地中海に注いでいます。</font><br>**指示**: 上記の取得文書を要約してください。<br>**回答**: ナイル川は<font color="red">アフリカ中部の山岳地帯を源流とし</font>、地中海へと注いでいます。 | 要約では「ナイル川は中央アフリカの山岳地帯を源流とする」と誤って記載されていますが、本文ではナイル川の源流は中央アフリカの大湖地域であると明記されています。 |
| | 論理的矛盾 | **指示**: 方程式 $2x+3=11$ を段階的に解いてください。<br>**回答**: 第1段階: 両辺から3を引くと $2x=8$ となります。<br>第2段階: 両辺を2で割ると <font color="red">$x=3$</font> となります。<br>したがって、方程式の解は <font color="red">$x=3$</font> です。 | 第1段階の計算は正しく、$x$ を含む項を分離して $2x=8$ と導いています。しかし、第2段階の計算に誤りがあり、8を2で割った結果を $x=3$ としていますが、これは第1段階の計算結果と矛盾しています。 |

# 3. LLMの幻覚発生要因

LLMの幻覚発生要因は多岐にわたり、LLMが能力を獲得する全過程に及んでいます。本節では、LLMにおける幻覚発生の根本要因について、主に以下の3つの側面から詳細に分析します：（1）データ要因（3.1）、（2）学習プロセス要因（3.2）、および（3）推論プロセス要因（3.3）。

## 3.1. データに起因する幻覚

大規模言語モデル（LLM）の学習データは主に2つの要素から構成されます：（1）事前学習データ、これはLLMが汎用的な能力と事実に関する知識を獲得する基盤となる[388]、（2）アライメントデータ、これはLLMがユーザーの指示に従い、人間の嗜好に沿った出力を生成するよう調整するためのデータです[322]。これらのデータはLLMの能力範囲を継続的に拡大する一方で、意図せずしてLLMの幻覚生成の主要な要因となっています。この現象は主に以下の3つの側面で顕著に現れます：

- **3.1.1. 誤った情報やバイアスが含まれる事前学習データソースの欠陥**
- **3.1.2. 事前学習データの範囲によって必然的に制限される知識範囲**
- **3.1.3. 質の低いアライメントデータによって誘発される幻覚**

### 3.1.1. 誤った情報とバイアス

ニューラルネットワークには、学習データを記憶する本質的な傾向[35]があり、この記憶傾向はモデルの規模が大きくなるにつれて増大します[34, 54]。一般的に、こうした本質的な記憶能力は、幻覚との戦いにおいて両刃の剣となります。一方で、LLMが学習データを記憶する能力は、彼らが深遠な知識を獲得する上で重要ですが、他方で、事前学習データに含まれる誤情報やバイアスの問題も生じます。これらのバイアスは意図せず増幅され、模倣による虚偽情報[182]の拡散や社会的バイアスの強化といった形で現れる可能性があります。より包括的な理解のために、詳細な事例を表2に示します。

- **模倣的虚偽**: 偽情報や根拠のない噂といった誤った情報は、ソーシャルメディアプラットフォーム上で広く拡散しており、大規模言語モデル（LLM）の幻覚現象の主要な要因となっています。事前学習用の大規模コーパスに対する需要の高まりに伴い、ヒューリスティックなデータ収集手法の採用が不可欠となっています。膨大な量のデータ取得を可能にする一方で、データ品質の一貫性維持が課題となり、これが事前学習データへの誤った情報の混入を招く要因となっています[20, 328]。このような状況下では、LLMがその優れた記憶能力ゆえにこうした虚偽情報を生成する可能性が高まり、結果として模倣的虚偽が生じることになります。この問題をさらに深刻化させているのは、LLMがコンテンツ作成の障壁を大幅に低減させたことで、公共の言論やインターネットエコシステムの信頼性に対して重大なリスクをもたらしている点です。
- **社会的バイアス**: 誤った情報に加え、社会的バイアスもソーシャルメディアプラットフォームに深く根付いており、採用におけるバイアス、ニュース報道における偏見、ヘイトスピーチの拡散といった形で様々な形で現れます。純粋に否定的な現象として、バイアスと幻覚は明らかに異なる性質を持ち、主にステレオタイプの拡散や社会的不平等として顕在化します。ただし、特定のバイアスは幻覚と本質的に関連しており、特にジェンダー[242]や国籍[155, 226]に関連するものが挙げられます。例えば、LLMは看護という職業を女性と関連付ける傾向があり、ユーザー提供の文脈で性別が明示されていない場合でもこの傾向が見られます。これはセクション2.3で議論されている文脈不一致の一例です。このようなバイアスは、多様なバイアスのかかった見解が蔓延するインターネットベースのテキストから意図せず獲得され、生成コンテンツに伝播される可能性があります[155]。

**表2. 誤情報やバイアスに起因する幻覚現象の事例分類**

| 分類 | 説明 | 入力 | モデル出力 | 解説 |
| :--- | :--- | :--- | :--- | :--- |
| **模倣による虚偽** | モデルの学習データにおいて、頻繁に言及されながらも誤った事実が複数回登場しています：トーマス・エジソンが電球を発明したという記述です。 | 電球の発明者として認められている人物は誰ですか？ | トーマス・エジソンが電球の発明者として認められています。 | エジソンは電球の設計改良に貢献しましたが、単独での発明者ではありません。モデルの回答には、学習データに頻出する誤解が反映されており、これは模倣による虚偽の典型例と言えます。 |
| **社会的バイアス** | モデルの学習データにおいて、特定の名前が特定の国籍と頻繁に関連付けられている場合があります。 | 以下のニュース記事の要約をお願いします：キム博士が量子コンピューティングに関する画期的な論文を発表しました。この論文では新たなアルゴリズムとその実世界応用における可能性について詳述されています。専門家から革命的な成果として高く評価されています。 | キム博士（韓国出身）は量子コンピューティングに関する論文を発表し、新たなアルゴリズムとその応用について詳述しています。 | モデルは「キム」という名前に基づいて「韓国」という国籍を誤って付加しており、特定の名前と国籍を関連付けるバイアスが存在していることが明らかです。このバイアスは要約における幻覚現象を引き起こす可能性があります。 |

### 3.1.2. 知識の限界範囲

事前学習コーパスが膨大な事実知識をLLMに付与する一方で、それらには本質的な知識限界範囲が存在します。この限界は主に以下の2つの要因に起因します：
1. LLMが事前学習過程で遭遇する全ての事実知識、特に出現頻度の低い「ロングテール」知識を記憶しきれないという限界。
2. 事前学習コーパス自体に内在する限界範囲で、これは急速に変化する世界知識や、著作権法によって制限されるコンテンツを含んでいないことに起因します。

その結果、LLMがその限られた知識範囲外の情報に遭遇した場合、幻覚を生成する傾向が強まります。この現象を明瞭に示す具体例を表3に示します。

- **ロングテール知識**: 事前学習コーパス内の知識分布は本質的に非均一であり、これがLLMが様々な種類の知識に対して異なる習熟度を示す原因となっています。最近の研究では、モデルの一般領域質問に対する正解率と、事前学習コーパス内の関連文書量[145]あるいはエンティティの人気度[204]との間に強い相関関係があることが示されています。さらに、LLMは主に広範な一般領域コーパス[93, 243, 254]で訓練されているため、領域特化型知識において不足が生じる可能性があります。この限界は、LLMが医療[179, 279]や法律[149, 353]分野の質問など、領域特化型専門知識を必要とするタスクに直面した場合に特に顕著に現れます。この場合、これらのモデルは顕著な幻覚を示す傾向があり、多くの場合、事実の捏造として現れます。
- **最新知識への対応**: ロングテール知識の不足に加え、LLM内の知識範囲に関する本質的な制約として、最新知識への対応能力が限られている点が挙げられます。LLMに組み込まれた事実知識には、明確な時間的境界が存在します。これらのモデルは一度学習が完了すると、その内部知識が更新されることはありません。この問題は、私たちの世界が常に変化し、進化し続けているという事実を考慮すると特に深刻です。時間的範囲を超えた質問に直面した場合、LLMはしばしば事実を捏造したり、かつては正しかったが現在では時代遅れとなった回答を提供したりすることがあります。
- **著作権保護対象の知識**: ライセンス制限[262]のため、既存のLLMは法的に、公開ライセンス[63, 93]のコーパスや著作権法に抵触しない形で利用可能なコーパスのみで学習することが法的に制約されています[10, 115]。この制約は、LLMが法的に取得できる知識の範囲と多様性に重大な影響を及ぼします。貴重な知識のかなりの部分――最近の科学的研究、専有データ、著作権保護対象の文学作品などの著作権保護対象資料に包含される知識――は、LLMにとって利用できない状態にあります。この除外は知識のギャップを生み、LLMが訓練データにアクセスできない領域で情報を生成しようとする際に、幻覚現象を引き起こす可能性があります[215]。

**表3. 知識境界の具体例**
*(赤色で強調表示された内容は幻覚的な出力結果を示します)*

| 分類 | 入力文 | モデルの出力 | 解説 |
| :--- | :--- | :--- | :--- |
| **希少疾患知識** | フェニルケトン尿症の原因は何ですか？ | フェニルケトン尿症は、<font color="red">ヘモグロビン遺伝子の変異によって引き起こされます。この変異はヘモグロビンとの特異的な相互作用を引き起こし、血液中での酸素運搬機能に影響を及ぼします。</font> | フェニルケトン尿症の正確な原因はPAH遺伝子の変異によるものです。このような専門的な医学的知識は、学習コーパスで遭遇する頻度が低く、LLMがこの情報を記憶するのに苦労する原因となります。 |
| **最新知識の把握** | 民主党は2024年アメリカ大統領選挙の民主党候補を誰に決定したのでしょうか？ | <font color="red">ミシェル・オバマ氏が立候補を表明し、世論調査でも首位に立っています。バラク・オバマ元大統領やバーニー・サンダース上院議員など、民主党の主要人物からも大きな支持を得ています。また、医療制度改革や気候変動対策を中心とした政策方針も明確に示しています。</font> | 2024年7月21日、ジョー・バイデン大統領が選挙戦からの撤退を表明した後、カマラ・ハリス氏を民主党候補として支持することを表明しました。このような幻覚が生じるのは、LLMのパラメトリック知識がリアルタイムで更新できないため、最新の出来事に関する虚偽情報が生成される可能性があるためです。 |
| **著作権保護対象知識情報** | 2023年に出版された小説『錬金術師の秘密』の結末について描写してください。 | <font color="red">『錬金術師の秘密』の劇的な結末では、主人公がついに賢者の石の真の秘密を解き明かし、すべての登場人物が知恵を共有することで平和的な解決を迎えます。</font> | 小説『錬金術師の秘密』は著作権で保護されており、LLMはこのような著作権保護対象資料から直接学習するようには設計されていません。そのため、モデルの出力には当該小説の結末に関する虚偽情報が含まれています。 |

### 3.1.3. 質の低いアライメントデータ

事前学習段階を経たLLMは、そのパラメータ内に膨大な事実知識を組み込み、明確な知識の境界を形成しています。教師あり微調整（SFT）段階では、LLMは通常、人間のアノテータによってラベル付けされた指示ペアを用いて訓練されるため、事前学習時に確立された知識境界を超える新たな事実知識が導入される可能性があります。Gekhmanら[98]は、SFTプロセスにおける新たな事実知識の統合に関する訓練ダイナミクスを分析し、LLMがこのような新たな知識を効果的に獲得することに困難を示すことを明らかにしました。最も重要な発見として、SFTを通じた新たな知識の獲得と幻覚現象の増加との間に相関関係が認められ、新たな事実知識の導入がLLMの幻覚傾向を促進することが示唆されました。さらに、Liら[168]は幻覚生成に対する指示文の影響について詳細な分析を行いました。その結果、主にタスク形式の学習に焦点を当てたタスク固有の指示文は、幻覚的な応答の割合が高くなる傾向が認められました。さらに、過度に複雑で多様な指示文も幻覚現象の増加を招くことが明らかになりました。

## 3.2. 訓練に起因する幻覚

セクション2.2で詳述したように、事前学習はLLMの基礎段階であり、主にGPT[29, 251, 252]によって確立されたトランスフォーマーアーキテクチャを基盤とし、OPT[372]、Falcon[243]、Llama-2[300]などによってさらに発展してきました。この段階では、因果関係に基づく言語モデリングが目的とされ、モデルは先行するトークンのみに基づいて後続トークンを予測するように学習します。この学習は単方向かつ左から右への逐次的な処理で行われます。この学習方法は効率的な訓練を可能にする一方で、複雑な文脈依存関係を捉える能力に本質的な限界をもたらし、幻覚現象の発生リスクを高める可能性があります[180]。さらに、最近の研究では、LLMが長距離および短距離の依存関係にまたがる予測不能な推論幻覚を示す場合があることが明らかになっており、これはソフトアテンション[52, 111]の限界に起因する可能性がある。この場合、アテンションはシーケンス長の増加に伴って位置間で希薄化します。特に、曝露バイアス[21, 256]の現象は、自己回帰生成モデルにおける訓練と推論の不一致に起因する長年の深刻な幻覚現象への寄与要因となっています。このような不一致は、特に自己回帰生成モデルにおいて、訓練と推論の不一致が幻覚現象[313]を助長します。

SFT（Supervised Fine-tuning）プロセスにおいて、LLMは事前学習時に設定された能力の限界に直面します。SFTの目的は、指示データとそれに対応する応答を活用することで、こうした事前獲得された能力を解放することにあります。しかし、注釈付き指示の要求がモデルの事前定義された能力限界を超える場合、課題が生じます。このような状況下では、LLMは実際の知識範囲を超えて応答を生成するように訓練されます。セクション3.1.3で論じられているように、新たな事実的知識に対する過剰適合は、内容を捏造しがちなLLMをさらに助長し、幻覚現象のリスクを高めます[98, 269]。さらにもう一つの重要な要因として、モデルが拒否できないという能力の欠如が挙げられます。従来のSFT手法では、モデルに各応答を完成させるよう強制する傾向があり、不確実性を正確に表現することを許していません[341, 362]。その結果、これらのモデルは知識範囲を超えるクエリに直面した場合、不確実性を拒否するのではなく、内容を捏造する傾向が強くなります。この知識範囲の不整合と、不確実性を正確に表現できないという能力の欠如は、SFT段階において幻覚現象が発生する主要な要因となっています。

### 3.2.3. RLHFに起因する幻覚現象

複数の研究[13, 31]により、LLMの活性化パターンには、その生成文の真実性に関する内部的な信念が反映されていることが明らかになっています。しかしながら、こうした内部的な信念と生成出力の間には時折不一致が生じることがある。たとえLLMが人間のフィードバックを用いて改良された場合[233]であっても、時折内部的な信念から逸脱した出力を生成することがあります。このような現象は「迎合行動[64]」と総称され、モデルが人間の評価者を喜ばせる傾向にあり、しばしば真実性を犠牲にしていることを示唆している。最近の研究によれば、RLHFによって訓練されたモデルには、ユーザーの意見に迎合する顕著な行動傾向が見られる。このような迎合行動は、明確な正解が存在しない曖昧な質問[245]、例えば政治的立場などに限らず、モデルがその不正確さを認識しているにもかかわらず、明らかに誤った回答を選択する場合にも生じることがある[327]。この現象をさらに掘り下げたSharmaら[274]は、迎合行動の根源はRLHFモデルの訓練プロセスにある可能性を示唆している。この行動における人間の選好の役割をさらに探求した結果、迎合行動の傾向は、人間と選好モデルの双方が、真実性よりも迎合的な回答に偏る傾向によって駆動されている可能性が高いことが示された。

## 3.3. 推論に起因する幻覚

デコーディング処理は、事前学習とアライメントが完了した後のLLMの能力発揮において重要な役割を果たします。しかしながら、デコーディング戦略における特定の不備は、LLMの幻覚現象を引き起こす要因となり得ます。

### 3.3.1. 不完全なデコーディング戦略

LLMは、非常に創造的で多様なコンテンツを生成する優れた能力を示しており、この能力はデコーディング戦略におけるランダム性の重要な役割に決定的に依存しています。確率的サンプリング[84, 118]は現在、こうしたLLMで採用されている主流のデコーディング戦略です。デコーディング戦略にランダム性を取り入れる理由は、高確率のシーケンスが驚くほど質の低いテキストを生成するという現象、すなわち「確率トラップ」[118, 209, 283, 363]が存在するという認識に基づいています。デコーディング戦略におけるランダム性がもたらす多様性には代償が伴い、幻覚現象のリスク増加と正の相関関係があります[59, 78]。サンプリング温度を上昇させるとトークンの確率分布がより均一になり、分布の尾部に位置する低頻度トークンをサンプリングする可能性が高まります。その結果、低頻度トークンをサンプリングする傾向が強まることで、幻覚現象のリスクがさらに増大します[5]。

### 3.3.2. 過剰自信現象

条件付きテキスト生成に関する先行研究[45, 212]では、生成途中のテキストに過度に集中することから生じる「過剰自信」現象が指摘されている。言語モデル、特に因果関係モデルのアーキテクチャを採用するLLMは広く利用されているものの、この過剰自信現象は依然として顕著に観察される。生成プロセスにおいて、次の単語の予測は言語モデルの文脈と生成途中のテキストの両方によって条件付けられる。しかし先行研究[19, 189, 307]で示されているように、言語モデルはその注意機構において局所的な集中傾向を示し、隣接する単語を優先するため、文脈への注意が著しく欠如する[275]。さらにこの問題は、長文で包括的な応答を生成する傾向が強いLLMにおいてさらに深刻化する。このような場合、指示内容を忘れるリスク[46, 193]に対する脆弱性がさらに高まる。この注意不足は、モデルが元の文脈から逸脱した内容を出力する「忠実性の幻覚」現象に直接寄与する。

### 3.3.3. Softmax Bottleneck

多くの言語モデルでは、softmax層が言語モデルの最終層の表現と単語埋め込みを組み合わせて、単語予測に関連する最終的な確率を計算します。しかしながら、softmaxベースの言語モデルの性能は、Softmax bottleneckとして知られる既知の制約によって制限されています[342]。この制約では、softmaxを分散型単語埋め込みと併用することで、文脈から導出される出力確率分布の表現力が制限され、言語モデルが所望の分布を生成できなくなるという問題が生じます。さらに、ChangとMcCallum[38]は、出力単語埋め込み空間内の所望の分布が複数のモードを示す場合、言語モデルがすべてのモードから上位の次単語を正確に優先付けすることに困難を抱えることを発見しました。これは幻覚現象のリスクも導入することになります。

### 3.3.4. 推論の失敗

ロングテール知識に関連する課題に加え、知識の効果的な活用は推論能力と不可分に結びついています。例えば、マルチホップ質問応答シナリオにおいて、LLMが必要な知識を有していても、質問と複数の関連性が存在する場合、推論能力の限界により正確な結果を生成することが困難になる場合があります[386]。さらに、Berglundら[22]は、LLMにおける特定の推論失敗現象を「逆転の呪い」として明らかにしました。具体的には、モデルが「AはBである」という形式で質問された場合には正しく回答できるのに対し、「BはAである」という逆形式の質問に対しては論理的推論が失敗するという現象が見られます。この推論の不一致は、単純な推論を超えた範囲にも及んでいます。

# 4. 幻覚検出手法と評価ベンチマーク

LLMにおける幻覚現象は近年大きな注目を集めており、LLMの信頼性や実応用における展開可能性に対する懸念を引き起こしています。LLMが人間らしいテキストを生成する能力を高めるにつれ、生成された内容が事実に基づくものか幻覚によるものかを正確に区別することがますます重要になっています。さらに、LLMにおける幻覚の程度を効果的に測定することは、その信頼性向上にとって極めて重要です。本節では、幻覚検出手法（4.1）とLLMの幻覚現象を評価するためのベンチマーク手法（4.2）について詳細に解説します。

## 4.1. 幻覚検出

LLMにおける幻覚検出の既存手法は、主に2種類の幻覚タイプに基づいて分類されます：
1. **事実性幻覚検出**: LLMの出力に含まれる事実的な誤りを特定することを目的としています。
2. **忠実性幻覚検出**: モデルの出力が与えられた文脈情報に対してどれだけ忠実であるかを評価する手法です。

### 4.1.1. 事実性幻覚検出

事実性幻覚検出では、LLMの出力が現実世界の事実と整合しているかどうかを評価します。一般的な手法は通常、2つのカテゴリに分類されます：
- **事実確認**: 生成された応答の事実的正確性を、信頼できる知識情報源との比較検証によって行うプロセスです。
- **不確実性推定**: 内部的な不確実性シグナルに基づいて事実不整合の検出に焦点を当てます。

**事実検証について**
LLMの出力は通常包括的な内容であり、複数の事実記述から構成されるため、事実検証のアプローチは一般的に以下の2つの主要ステップに分けられます：
1. **事実抽出**: モデルの出力内に含まれる独立した事実記述を抽出するプロセスです。
2. **事実検証**: これらの事実記述の正確性を信頼できる知識情報源と照合して検証するプロセスです。

検証に用いる知識情報源の種類に応じて、事実検証の方法論は大きく以下の2つの異なる部分に分類できます：
- **外部情報参照**: 長文生成テキストの事実確認において、最も直感的な手法は外部情報参照です。Minら[216]はFACTSCOREを開発しました。これは、長文生成テキストの評価に特化した詳細な事実確認指標です。まず生成内容を原子レベルの事実に分解し、続いて信頼性の高い知識源に基づいてその割合を計算します。この概念を発展させたChernら[50]は、証拠収集専用の外部ツール群を活用して、LLMが事実誤認を特定できる統一フレームワークを提案した。さらに、Huoら[128]はクエリ拡張によって検索プロセスを改善した。元の質問とLLMが生成した回答を組み合わせることで、話題の逸脱問題に対処し、検索された証拠が質問とLLMの回答の両方と整合するようにした。
- **内部検証**: パラメータに膨大な事実知識が符号化されているLLMは、事実確認のための事実知識源として活用されてきました。Dhuliwalaら[74]は検証連鎖（CoVe）を導入した。これはまず、LLMがドラフト回答に対する検証質問を生成し、続いてパラメータ知識を活用して回答の一貫性を元の回答と照らし合わせて評価することで、潜在的な不整合を検出するものである。Kadavathら[143]とZhangら[375]は、ブール質問への回答の事実性を評価する確率 $p(	ext{True})$ を計算し、モデルの内部知識のみに依存している。さらに、Liら[168]は、ほとんどの原子レベルの記述が相互に関連しており、あるものが他の記述の文脈的背景として機能する場合があり、これが誤った判断につながる可能性があることに注目した。そこで彼らは、LLMに対して、すべての事実記述を考慮した上で、幻覚判定を直接予測するよう指示した。ただし、LLMは本質的に信頼性の高い事実データベースではない[385]ため、事実確認のためにLLMのパラメータ知識のみに依存すると、不正確な評価につながる可能性がある。

**不確実性推定手法**
LLMの幻覚検出において外部知識ベースを用いた事実確認に依存する手法が多い中、本研究ではゼロリソース環境下でこの問題に対処する手法を提案し、情報検索の必要性を排除しています。これらの手法の基礎となる考え方は、LLMの幻覚発生原因が本質的にモデルの不確実性に起因するという前提です。したがって、モデルが生成した事実情報の不確実性を推定することで、幻覚検出が可能となります。不確実性推定手法は大きく2つのアプローチに分類できます：LLMの内部状態に基づく手法と、LLMの振る舞いに基づく手法です（図2参照）。

- **LLMの内部状態**: LLMの内部状態は、その不確実性を示す有用な指標となり得る。具体的には、トークンの出現確率やエントロピーといった指標を通じて、不確実性を定量化できる。Varshneyら[306]は、モデルが特定の概念に対して示す不確実性を、当該概念内で最も低いトークン出現確率を定量化することで測定した。その理論的根拠は、低い出現確率がモデルの不確実性を強く示す指標となる一方で、概念内に存在する高い出現確率のトークンからの影響は相対的に小さいというものである。同様に、Luoら[198]は、自己評価に基づくアプローチを用いて不確実性推定を行い、言語モデルが生成した概念を元の形に巧みに再構築する能力に根拠を置くアプローチを採用した。

![図2. 事実に基づく幻覚検出における不確実性推定手法の分類](img-1.jpeg)
*a) LLMの内部状態と b) LLMの挙動を含み、LLMの挙動は主に2つのカテゴリに分類される：自己整合性と多者間討論。*

- **LLMの挙動**: ただし、API経由でのみアクセス可能なシステム[100, 214, 231]の場合、出力トークンの確率分布情報が利用できないことがあります。このような制約条件下で、複数の研究がモデルの不確実性を調査する方向に焦点を移しています。これは自然言語プロンプト[143, 335]を使用する方法や、モデルの行動的発現を調べる方法によって実現されています。例えば、同じプロンプトに対してLLMから複数の応答をサンプリングすることで、Manakulら[205]は事実記述間の一貫性を評価する手法によって幻覚現象を検出しました。ただし、これらの手法は主にモデルに直接情報要求や検証を求める直接的なクエリに依存しています。Agrawalら[3]は調査インタビューに着想を得て、間接的なクエリの使用を提唱しました。直接的なクエリとは異なり、これらの間接的なクエリはしばしば開かれた質問形式で特定の情報を引き出すことを目的としています。これらの間接的なクエリを使用することで、複数のモデル生成にわたる一貫性をより適切に評価できます。単一のLLMの複数生成にわたる自己一貫性を評価するだけでなく、追加のLLMを組み込むことでマルチエージェント視点を採用することも可能です。Cohenら[62]は法的反対尋問の実践に着想を得て、LMvLMアプローチを導入しました。この手法では、尋問者LMが被尋問者LMに質問を投げかけ、複数回の対話を通じて主張の不整合を明らかにすることを目的としています。

### 4.1.2. 忠実性に基づく幻覚検出

大規模言語モデル（LLM）が文脈やユーザー指示を正確に再現することは、会話型検索から対話型システムまで、IRアプリケーションにおける実用的な有用性を確保する上で極めて重要である。我々は忠実性評価に特化した既存の幻覚検出指標を以下の5つのカテゴリに分類し、概要を図3に示します：
1. 事実ベース
2. 分類器ベース
3. QAベース
4. 不確実性ベース
5. LLMベース

![図3 忠実性評価手法の概要](img-2.jpeg)
*a) 事実ベース評価指標は、生成コンテンツと原典コンテンツ間の事実の重複度合いを測定することで忠実性を評価する。b) 分類器ベース評価指標は、訓練済み分類器を用いて生成コンテンツと原典コンテンツ間の含意関係の度合いを判定する。c) 質問応答ベース評価指標は、質問応答システムを活用して原典コンテンツと生成コンテンツ間の情報整合性を検証する。d) 不確実性推定は、モデルが生成した出力に対する確信度を測定することで忠実性を評価する。e) プロンプトベース評価指標では、LLMを評価者として誘導し、特定のプロンプト戦略を用いて生成コンテンツの忠実性を評価する。*

- **事実ベース指標**: 忠実性評価において、最も直感的な手法の一つは、生成コンテンツとソースコンテンツ間の重要な事実の重複度合いを測定することである。事実の多様な現れ方を考慮すると、忠実性はn-gramに基づいて測定可能である。従来のn-gramベース評価指標（BLEU[239]、ROUGE[181]、PARENT-T[324]など）は、生成コンテンツと原典コンテンツ間の微妙な差異を区別する点で限界がある[208]。エンティティベース評価指標[225]は、エンティティの重複度合いを計算することでさらに一歩進んだ評価手法であり、これらの主要なエンティティの省略や不正確な生成は忠実性を欠いた応答につながる可能性がある。特に、エンティティが一致していても、それらの間の関係が誤っていることがあり得る。したがって、関係ベース評価指標[99]は関係トリプルの重複度合いに焦点を当て、訓練済みのエンドツーエンド事実抽出モデルを用いて抽出された関係トリプルの重複度合いを計算する指標を導入する。

- **分類器ベースの評価指標**: 事実の重複を計算するだけでなく、モデル生成の忠実性を評価する別の直接的なアプローチとして、自然言語推論（NLI）や事実検証などの関連タスクから収集したデータで訓練した分類器を利用する方法がある。また、タスク固有の幻覚的コンテンツと忠実なコンテンツの両方を含む合成データで訓練した分類器を利用する方法もある。生成テキストの忠実性を評価する基盤的な原則は、真に忠実なコンテンツはその情報源コンテンツから必然的に導出されるという考え方に基づいている。この考え方に沿って、多くの研究[82, 208]では、NLIデータセットで訓練した分類器を用いて事実の不正確さを特定しており、特に抽象的な要約タスクの文脈において有用である。しかし、Mishraら[217]は、従来のNLIデータセットと不整合検出データセットの間で入力粒度が一致していないため、不整合を効果的に検出する能力が制限されることを指摘している。これを受けて、より先進的な研究では、敵対的データセットでのファインチューニング[17]、依存関係アークレベルでの含意判断の分解[101]、文書を文単位に分割した後文ペア間でスコアを集約する方法[154]など、より高度な手法が提案されている。関連タスクから収集したデータを用いて分類器をファインチューニングする方法は、忠実性を評価する上で有望な結果を示しているが、関連タスクと下流タスクの間には本質的なギャップが存在することを認識することが重要である。注釈付きデータの不足は、これらの手法の適用可能性をさらに制限する要因となっている。この課題に対応するため、データ拡張手法を活用して合成データを構築し、分類器のファインチューニングに利用する研究が急増している。この手法は、ルールベースの摂動[79, 152, 266]や生成[389]などの手法を用いて合成データを構築する。

- **QAベースの評価指標**: 分類器ベースの指標とは対照的に、近年注目を集めている。これらの指標[77, 119, 271, 310]は、LLMの生成内容とその出典情報との間の情報重複をより効果的に捉える能力を備えている点で特徴がある。これらの指標は、まずLLMの出力に含まれる情報単位から対象回答を選択し、次に質問生成モジュールによって質問を生成する。生成された質問は、ユーザーの文脈に基づいて出典回答を生成するために使用される。最終的に、LLMの応答の忠実性は、出典回答と対象回答の間のマッチングスコアを比較することで算出される。これらの方法論は共通のテーマ的アプローチを共有しているものの、回答選択、質問生成、回答重複などの側面において差異が存在し、これが多様な性能結果をもたらしている。この基礎研究を基盤として、Fabbriら[80]はQAベースの評価指標の構成要素について詳細な評価を実施し、これにより忠実性評価のさらなる向上がもたらされた。

- **不確実性に基づく評価指標**: 事実性の幻覚を検出するために用いられる不確実性ベースのアプローチ（セクション4.1.1）と同様に、忠実性評価における不確実性推定も広く研究されており、通常はエントロピーと対数確率によって特徴づけられる。エントロピーベースの不確実性については、Xiaoら[333]が深層アンサンブル[156]によって推定される予測不確実性とデータ-to-テキスト生成における幻覚発生可能性との間に正の相関関係があることを明らかにしている。関連研究として、Guerreiroら[106]はニューラル機械翻訳において、Monte Carlo Dropout[92]が生成する仮説の分散を不確実性指標として採用している。より最近では、van der Poelら[305]が条件付きエントロピー[337]を用いて抽象要約におけるモデルの不確実性を評価している。対数確率については、単語レベルや文レベルといった異なる粒度レベルで適用可能である。特に、複数の研究[91, 106, 359]では、長さ正規化済みの系列対数確率を用いてモデルの信頼度を測定している。さらに、幻覚情報が先行文脈内に存在する場合に同じ幻覚情報に高い確率が割り当てられるという特性に着目し、Zhangら[374]は最も情報量が多く重要なキーワードに焦点を当て、幻覚コンテンツの伝播を抑制するペナルティ機構を導入した。

- **LLMベースの評価**: 近年、LLMの顕著な指示追従能力が、自動評価[51, 190, 314]におけるLLMの潜在的な有用性を強調している。この能力を活用し、研究者らはモデル生成コンテンツの忠実性を評価するための新たなパラダイムを模索している[2, 95, 133, 153, 199]。LLMに対して具体的な評価ガイドラインを提供し、モデル生成コンテンツとソースコンテンツの両方を与えることで、効果的に忠実性を評価できる。最終的な評価出力は、忠実性に関する二値判定[199]、あるいは忠実度の程度を示すkポイントLikert尺度[95]のいずれかとなる。プロンプト選択に関しては、評価プロンプトは直接プロンプト、思考連鎖プロンプト[2]、文脈内学習[133]の使用、あるいはモデルが説明を伴う評価結果を生成することを許容する[153]など、様々なアプローチが可能である。

## 4.2. 幻覚生成ベンチマーク

本節では、既存の幻覚検出ベンチマークについて包括的に解説します。これらは大きく分けて2つのカテゴリに分類されます：
1. **幻覚評価ベンチマーク（4.2.1）**: 最先端の大規模言語モデル（LLM）が生成する幻覚の範囲を評価するものです。
2. **幻覚検出ベンチマーク（4.2.2）**: 既存の幻覚検出手法の性能を評価するために特別に設計されたベンチマークです。

これらのベンチマークを統合することで、LLMにおける幻覚パターンを精緻かつ包括的に探求するための統一的な枠組みが確立されます。

**表4. 既存のハルシネーション評価ベンチマークの概要**
*(属性、事実性、忠実性はそれぞれ、ベンチマークがLLMの事実性評価または忠実性ハルシネーション検出に用いられるかどうかを示しており、手動入力はデータセット内の入力が手書きであるかどうかを示しています)*

| ベンチマーク | データセット | データサイズ | 言語 | 属性 | タスク種別 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| | | | | **事実性** | **忠実性** | **手動** | **タスクタイプ** | **入力** | **ラベル** | **評価指標** |
| TruthfulQA [148] | - | 817 | 英語 | ✓ | | ✓ | 生成型質問応答 | ✓ | ✓ | EM & F1 |
| SelfCheckGPT-Wikibio | - | 1,908 | 英語 | | ✓ | | | | | |
| HaloEval [305] | タスク特化型 | 30,000 | 英語 | ✓ | ✓ | | | | | |
| FACTOR | Wiki-FACTOR | 2,994 | 英語 | ✓ | | | | | | |
| BAMBOO | SenHallo | 200 | 英語 | ✓ | ✓ | | | | | |
| HaloQA [311] | 誤解を招く | 175 | 中国語 | ✓ | | ✓ | | | | |
| | 知識レベル | 206 | 中国語 | ✓ | | ✓ | | | | |
| | 変動が激しい | 150 | 英語 | ✓ | | ✓ | | | | |
| PHD [42] | PHD-LOW | 100 | 英語 | ✓ | | | | | | |
| | PHD-High | 100 | 英語 | ✓ | | | | | | |
| | DROP | N/A | 英語 | ✓ | | | | | | |
| LSum [90] | - | 6,166 | 英語 | | ✓ | | | | | |
| | NQ-Open | 250 | 英語 | ✓ | | | | | | |
| | 科学 | 1,409 | 英語 | ✓ | | | | | | |

### 4.2.1. 幻覚検出ベンチマークの設計方針

LLMが生成する幻覚の傾向を定量的に評価するため、特に事実誤認や文脈からの論理的不整合に焦点を当てた幻覚検出ベンチマークが設計されている。LLMが高頻度で出現する知識を記憶する能力に長けていることを踏まえ、これらのベンチマークでは主にロングテール知識領域や難易度の高い質問に焦点を当て、模倣的な虚偽回答を誘発しやすいケースを評価対象としている。評価手法としては、複数選択肢形式のQAタスクを採用し、正解率に基づく指標で性能を評価する方法、あるいは生成型QAタスクを採用し、人間による判定または代理モデルによるスコアリングによって評価する方法が一般的である。

- **長期記憶型事実知識評価**: 長期記憶型事実知識評価は、Wikipediaから直接抽出したエンティティの人気度に基づいて構築されています。世界知識が絶えず進化する現状において、LLMの事実性評価においては最新の世界知識を検証することが極めて重要です。常に変化するベンチマークとしては、REALTIMEQA [148]とFreshQA [308]が特に注目されます。REALTIMEQAはリアルタイムで更新されるオープンドメインの多肢選択問題を提供し、これらの問題は最新のニュース記事から抽出されており、政治、ビジネス、スポーツ、エンターテインメントなど幅広い分野を網羅しています。FreshQAも同様に、モデルが誤った信念や誤解に基づいて回答しがちな「模倣的虚偽」を引き出すように設計されたベンチマークです。GPT-3によって正しく回答された問題を除外した結果、437問のフィルタリング済み問題が残りました。もう一つのパートには380問の非フィルタリング問題が含まれており、これらは敵対的手法を用いて作成されたものではありません。TruthfulQAの構築手法を踏襲し、HalluQAは中国語LLMにおける幻覚現象を評価するベンチマークとして特別に設計されており、模倣的虚偽と事実誤認に焦点を当てています。このベンチマークは30分野にわたる450問の手作業で作成された敵対的問題で構成されており、2つのパートに分類されています。誤解を招くセクションには、GLM-130Bを欺くことに成功した問題が含まれ、知識セクションにはChatGPTとPuyuの両方が一貫して誤った回答をする問題が保持されています。様々な分野におけるLLMの幻覚現象を総合的に評価するため、Liら[168]は[169]に基づき、改良版幻覚評価ベンチマークであるHaluEval 2.0を構築しました。このベンチマークには、生物医学、金融、科学、教育、オープンドメインの5分野にわたる8,770問の、LLMが幻覚を起こしやすい問題が含まれています。

### 4.2.2. 幻覚検出ベンチマークの現状

これまでの研究では、抽象要約[81, 102, 152, 208, 236, 310]、データからテキスト生成[240, 296]、機械翻訳[389]など、特定タスクにおける幻覚検出に主眼が置かれてきた。しかし、これらの研究で生成されるコンテンツは、BART[164]やPEGASUS[366]など、比較的能力の低いモデルをベースとしている場合が多い。このため、これらの研究で用いられる幻覚検出手法の有効性を正確に評価することは困難であり、LLM時代のより複雑なシナリオを包含するベンチマークの開発に向けた抜本的な転換の必要性が浮き彫りになっている。

例えば、SelfCheckGPT-Wikibio[213]は、GPT-3を用いて合成Wikipedia記事を生成し、事実性の観点から手動でアノテーションを施した文レベルのデータセットを提供しており、伝記分野における幻覚検出の課題を浮き彫りにしている。これと補完関係にあるHaluEval[169]は、自動生成と人間によるアノテーションを組み合わせることで、LLMが幻覚を認識する能力を評価する手法を提案している。5,000件の一般ユーザークエリと30,000件のタスク特化型サンプルを用いて、「サンプリング後フィルタリング」アプローチを採用しています。既存研究の多くが短文書を対象としているのに対し、BAMBOO [76]とScreenEval [158]は長文における幻覚検出の適用範囲を拡大しています。さらに、FELM [42]は、世界知識、科学、数学など多様な分野にわたる事実性評価を行うことで、他の研究とは一線を画しています。817件のサンプルが様々な事実性の側面について注釈付けされており、LLM生成コンテンツにおける事実性評価の分野横断的な必要性に応えています。別のアプローチとして、PHD [340]はWikipediaのエンティティ分析を通じて、長文における非事実的内容の段落レベル検出に焦点を当てており、LLMの知識深度に関するより精緻な見解を提供しています。RealHall [90]とSAC3 [364]は、オープンドメイン質問応答に焦点を当てた実世界アプリケーションと密接に整合しており、LSum [85]は要約タスクに特化した研究です。

# 5. 幻覚抑制手法の包括的レビュー

本節では、大規模言語モデル（LLM）における幻覚現象を抑制するための最新手法を体系的にレビューします。『幻覚原因分析』（セクション3）で考察した知見を踏まえ、本レビューでは幻覚の根本原因に基づいてこれらの手法を体系的に分類します。具体的には、データ関連の幻覚に対処する手法（5.1）、学習プロセスに関連する幻覚への対策（5.2）、および推論プロセスに関連する幻覚への対処法（5.3）について、それぞれ特有の課題に対処するための個別最適化されたアプローチを解説します。

## 5.1. データ関連のハルシネーション軽減

### 5.1.1. データフィルタリング

誤情報やバイアスの発生を抑制するため、信頼性の高いソースから厳選された高品質な事前学習用データを選択するという直感的なアプローチが有効である。これにより、データの事実精度を確保するとともに、社会的バイアスの混入を最小限に抑えることが可能となる。GPT-2の登場当初から、Radfordら[252]は、人間の専門家による厳格な選別とフィルタリングを経たウェブページのみを収集することの重要性を指摘していた。しかし、事前学習データセットが大規模化するにつれ、手動による選別は現実的ではなくなってきている。学術的あるいは専門的なドメインデータは通常事実精度が高いため、高品質なデータ収集が主要な戦略として浮上している。注目すべき事例として、Pile[93]や「教科書レベル」のデータソース[107, 177]などが挙げられる。さらに、事前学習段階で事実データのアップサンプリングを行うことは、LLMの事実精度向上に効果的であることが実証されており[300]、これによりLLMの幻覚現象を軽減することが可能となる。

データソースの厳密な管理に加え、重複除去は重要な処理工程である。既存の手法は主に完全重複と近似重複の2種類に分類される。
- **完全重複**: 最も単純な手法は部分文字列の完全一致による同一文字列の識別である。しかし、事前学習データの規模が膨大なため、この処理は計算資源を大量に消費する。より効率的な手法としては、接尾辞配列[206]の構築を利用することで、多数の部分文字列クエリを線形時間で効率的に計算できる。
- **近似重複**: 識別には通常、ハッシュベースの手法を用いた全文の近似マッチングが用いられ、n-gramの重複が大きい文書ペアを特定する。さらに、MinHash[28]は大規模重複除去タスク[110]で広く採用されているアルゴリズムである。また、SemDeDup[1]では、事前学習モデルから得られる埋め込み表現を利用して意味的重複を識別しており、これは同一ではないものの意味的に類似したデータペアを指す。

**議論**: データフィルタリングは幻覚の根本原因に直接対処するため、高品質で事実に基づいた情報源の利用を徹底することで、効果的に幻覚を抑制します。その有効性にもかかわらず、現在のデータフィルタリング手法の効率性と拡張性には、データ量の増大に伴って重大な課題が生じています。さらに、これらの手法はLLM生成コンテンツの影響をしばしば見落としがちであり、これが新たなリスクや不正確性をもたらす可能性があります。今後の進展のためには、データセットの急速な拡大とLLM生成コンテンツの複雑性に対応できる、より効率的で自動化されたデータフィルタリングアルゴリズムの開発に注力する必要があります。

### 5.1.2. モデル編集

モデル編集[280, 320, 369]は研究者からますます注目を集めており、追加知識を組み込むことでモデルの挙動を修正することを目的としています。現在のモデル編集技術は、主に「特定後編集」と「メタ学習」の2つのカテゴリに分類されます。

- **「位置特定後編集」手法**[66, 210]は2段階のプロセスで構成される。まずモデルパラメータの「不具合箇所」を特定し、次にそのパラメータを更新することでモデルの挙動を変更する。例えばROME[210]は、編集関連の層を特定するために活性化値を破壊・復元する手法を採用し、FFNのパラメータを直接更新することで知識編集を可能にした。MEMIT[211]はROMEと同様の知識特定手法を採用し、複数の層を同時に更新することで、数千もの編集知識を同時に統合することを可能にした。しかし、姚ら[347]の研究では、これらの手法には非自明な汎化能力が欠如しており、異なるモデルアーキテクチャに対する性能や適用性にもばらつきがあることが明らかになった。最も性能の高い手法であるROMEとMEMITは、実際にデコーダー専用LLMにおいてのみ良好な性能を発揮することが実証されている。
- **メタ学習手法**[70, 218]では、外部のハイパーネットワークを訓練して元のモデルの重み更新を予測します。しかし、メタ学習手法には通常、追加の訓練コストとメモリ要件が伴います。MEND[218]では、ハイパーネットワークのサイズを削減するため、特異な設計を施した低ランク分解を採用しています。特に注目すべきは、MENDがキャンセル効果を示す点です。異なるキーに対応するパラメータシフトが互いに大きく相殺し合うのです。MALMEN[292]ではこの問題に対処するため、パラメータシフトの集約を単純な総和ではなく最小二乗問題として定式化し、これにより大規模な編集処理能力を大幅に向上させています。これらの手法はモデルの挙動を微細に調整できるものの、パラメータの変更はモデルが本来有する知識に対して潜在的に有害な影響を及ぼす可能性があります。

**考察**: モデル編集は、大規模な再訓練を必要とせずに、特定の誤情報によって引き起こされる幻覚を軽減する精密な手法を提供します。しかし、これらの手法は大規模な更新処理に課題を抱え、特に継続的な編集が行われる場合、モデルの全体的な性能に悪影響を及ぼす可能性があります。したがって、今後の研究では、大規模な知識更新をより効率的に処理し、社会的バイアスによって引き起こされる幻覚に対処するため、モデル編集手法の改善に注力すべきです。

### 5.1.3. Retrieval-Augmented Generation (RAG)

典型的なRetrieval-Augmented Generation（RAG）[109, 165, 278]は、最初に外部情報源から関連知識を取得し、その後ユーザーのクエリと取得した文書の両方を条件としてジェネレータが最終的な応答を生成するという「取得→読解」パイプラインに従う。RAGはLLMから外部知識を分離することで、知識ギャップに起因する幻覚を効果的に軽減しつつ、LLMの性能に影響を与えない。一般的な実装方法は図4に示すように3つのカテゴリに分類される：一回限りの取得、反復的な取得、および事後的な取得であり、これらは取得のタイミングによって異なる。

![図4 検索拡張生成における3つの異なるアプローチ](img-3.jpeg)
*a) 一回限りの検索方式 - テキスト生成前に関連情報を一度だけ検索する方式 b) 反復検索方式 - テキスト生成プロセスにおいて複数回の検索を繰り返し行い、動的に情報を統合する方式 c) 事後検索方式 - 回答生成後に検索処理を行い、生成内容を精緻化・事実確認する方式。*

- **一回限りの取得**: 単一の取得で得られた外部知識をLLMのプロンプトに直接付加することを目的としている。Ramら[255]はIn-context RALMを導入しており、これはLLMの入力テキストを選択した文書で単純にかつ効果的に前置するという戦略である。Wikipediaなどの従来の知識リポジトリに加え、現在進行中の研究では代替的なアプローチとして、特に知識グラフ（KG）がLLMへのプロンプティングにおいて重要な役割を果たし、最新の知識との相互作用を促進しつつ、堅牢な推論経路を引き出すことができる[14, 249, 329]。Varshneyら[306]はパラメトリック知識誘導（PKG）フレームワークを提案しており、LLMに特定分野の知識を付与する機能を備えている。PKGは学習可能な背景知識モジュールを採用しており、これをタスク知識と整合させることで、関連する文脈情報を生成する。
- **反復的情報検索手法**: 多段階推論タスク[344]や長文質問応答[83, 284]のような複雑な課題において、従来の一度きりの情報検索では十分な性能が得られない場合があります。このような高度な情報ニーズに対応するため、近年の研究[113, 301, 346]では、推論プロセスの各段階で外部知識を継続的に取り込み、進行中の推論に基づいて情報検索プロセスをさらに誘導することで、推論チェーンにおける事実誤認を削減する手法が提案されています。連鎖的推論プロンプティングを基盤として、Pressら[247]はself-ask手法を導入しました。従来の継続的で明示的な連鎖的推論プロンプティングとは異なり、self-askでは各推論段階で解決すべき質問を明示的に定義し、その後に続く質問に基づいて検索アクションを実行します。情報検索プロセスの誘導に連鎖的推論プロンプトのみに依存するのではなく、Fengら[87]およびShaoら[273]は、モデルの応答をより関連性の高い知識を取得するための洞察に満ちた文脈として利用する反復的情報検索-生成協調フレームワークを採用しました。このフレームワークでは、モデルの応答が後続の反復において応答を精緻化するための洞察に満ちた文脈として機能します。多段階推論タスクを超えて、Jiangら[140]は長文生成に重点を移しました。彼らは、モデルベースのガイダンスを用いて今後の予測を逐次的に質問として扱い、関連する文書を取得するアクティブ情報検索拡張生成フレームワークを提案しました。予測に信頼度の低いトークンが含まれている場合、その文は再生成されます。さらに、Zhangら[371]はMixAlignを提案しました。これは、モデルベースのガイダンスを用いてユーザー質問を逐次的に精緻化し、ユーザーからの明確化を求めることで、質問と知識の整合性を高める手法です。
- **事後的情報検索**: 従来の「検索してから読む」というパラダイムを超えて、一連の研究が事後的情報検索の分野を探求し、モデルベースのガイダンスを用いてLLMの出力を逐次的に情報検索ベースの修正を行うことで精緻化しています。Gaoら[94]は、LLMの信頼性と推論の正確性を向上させるため、「調査→修正」ワークフローを採用した。まず関連する証拠を調査し、その後初期生成結果と証拠との間に検出された不一致に基づいて推論を修正する。同様に、Zhaoら[381]は外部知識を組み込むことで推論チェーンの事実精度を高める「検証→編集」フレームワークを提案した。平均よりも一貫性が低い推論チェーンに対しては、このフレームワークが検証質問を生成し、取得した知識に基づいて推論を修正することで、より事実に即した応答を保証する。Yuら[358]は、事後検索手法を多様な回答生成によって強化した。単一の回答を生成するだけでなく、様々な潜在的回答をサンプリングすることで、より包括的な検索フィードバックを可能にする。さらに、検索前後の回答の可能性を考慮するアンサンブル手法を採用することで、誤った検索フィードバックのリスクをさらに軽減している。

**考察**: 検索拡張生成手法の重要な利点の一つは、知識の不足に起因する幻覚現象を軽減できる点であり、さらにこの手法の汎用性により、あらゆる分野への適用が可能となる。この柔軟性は、外部知識ベースを検索プラグインのように扱い、必要に応じて交換・修正できるという手法のモジュール性によってさらに強化される。一方、欠点としては、関連性の低い検索結果の影響を受けやすく、これがノイズや不正確な情報を応答生成プロセスに導入することで、全体的な性能低下を招く可能性がある。さらに、この現行パラダイムでは、検索器と生成器コンポーネント間のインタラクションが浅くなる傾向があり、これが知識の最適利用を妨げる要因となっている。したがって、今後の研究では、関連性の低い検索結果の影響を最小限に抑えつつ、クエリの文脈や過去のインタラクション性能に基づいて検索戦略を動的に調整できる適応型学習コンポーネントを統合した、堅牢なRAGシステムの開発に注力すべきである。

## 5.2. 学習関連の幻覚現象の軽減

訓練プロセスにおける幻覚現象を軽減するための研究において、事前学習モデルに関連する幻覚現象の緩和策は主にモデルアーキテクチャの制約、特に単方向の表現処理と注意機構の不具合に焦点を当てて研究が進められてきた。この観点から、多くの研究チームがこれらの問題に対処するために特別に設計された新規モデルアーキテクチャの開発に取り組んできた。単方向表現処理の制約に対処するため、Liら[180]は双方向自己回帰アプローチを採用したBATGPTを提案した。この設計により、モデルはこれまでに観測されたすべてのトークンに基づいて次のトークンを予測できるようになり、過去と未来の文脈を考慮することで、両方向の依存関係を効果的に捉えられるようになった。この概念を発展させたLiuら[189]は、エンコーダ・デコーダモデルがそのコンテキストウィンドウをより効果的に活用する可能性を強調し、今後の大規模言語モデルアーキテクチャ設計における有望な方向性を示した。さらに、自己注意ベースのアーキテクチャにおけるソフト注意の限界を認識したLiuら[183]は、注意機構をシャープにする正則化項を提案した。このプラグ・アンド・プレイ方式では、微分可能な損失項を用いて自己注意アーキテクチャを指定する[365]ことで、スパース性を促進し、推論における幻覚現象を大幅に低減させることができた。

LLMの事前学習段階において、学習目的の選択はモデルの性能を決定づける重要な要素となります。しかし従来の学習目的では、断片的な表現やモデル出力の一貫性欠如といった問題が生じることがあります。近年の研究では、これらの課題を解決するため、事前学習戦略の改良を通じて、より豊かな文脈理解を実現し、バイアスの影響を回避する手法が開発されています。LLMの学習における本質的な制約として、文書レベルの非構造化事実知識が、GPUメモリ制約や計算効率の観点から、しばしば断片化されてしまうという問題があります。

Leeら[160]は、事実性を強化した訓練手法を提案した。事実に基づく文書の各文にTOPICPREFIXを付加することで、これらの文を独立した事実情報に変換し、事実誤認を大幅に低減するとともに、モデルの事実間関連性の理解能力を向上させる。同様に、事前訓練時に短い文書を無作為に連結すると、モデル出力に一貫性の問題が生じる可能性があることから、Shiら[276]はIn-Context Pretrainingという革新的な手法を提案した。この手法では、LLMを関連する文書群の連続したシーケンスで訓練する。文書の順序を変更することで、この方法は文脈ウィンドウ内の類似性を最大化することを目指す。明示的にLLMに文書境界を越えた推論を促すことで、生成結果間の論理的一貫性を強化する可能性がある。

**考察**: 事前訓練に関連する幻覚を軽減するために設計された戦略は、通常、基本的なアプローチであり、大幅な改善をもたらす可能性がある。しかし、これらの戦略は通常、事前訓練アーキテクチャと目的の修正を伴うため、計算資源的に負荷が高い。さらに、これらの統合手法は広範な適用性に欠ける場合がある。今後の方向性としては、システムの大幅な改修を必要とせず、普遍的に適用可能な適応的かつ効率的な戦略の開発に注力すべきである。

### 5.2.2. ハルシネーション軽減における整合性問題への対応

アライメント過程で生じるハルシネーションの多くは、能力整合性と信念整合性の問題に起因しています。しかし、LLMの知識範囲を明確に定義することは困難であり、LLMが本来有する能力と、人間によるアノテーションデータで提示される知識との間に生じるギャップを埋めることは容易ではありません。能力整合性の問題に起因するハルシネーションは、しばしば迎合的な応答として現れます。これは、LLMが人間の承認を得ようとするあまり、望ましくない形で迎合的な応答を選択する傾向があることを示しています。この迎合的な行動は、人間の好みに基づく判断が、より真実に近い応答よりも迎合的な応答を好む傾向があるという事実[274]に起因しており、これが報酬ハッキング[268]の道を開く要因となっています。この問題に対処するための単純な戦略として、人間の好みに基づく判断を改善し、それに伴って好みモデル自体も改善することが挙げられます。最近の研究[25, 268]では、LLMを活用して人間のラベラーが見落とした欠陥を特定する支援を行う方法が検討されています。さらに、Sharmaら[274]の研究では、複数の人間の好みを集約することでフィードバックの質が向上し、結果として迎合的な応答が減少することが明らかになっています。

さらに、LLMの内部活性化状態を操作することで、モデルの挙動を改変できる可能性も示されている。これはファインチューニング[327]や、推論時における活性化状態の制御[69, 117, 289]といった手法によって実現可能である。具体的には、Weiら[327]は合成データを用いた介入手法を提案しており、主張の真偽がユーザーの意見に依存しない合成データを用いて言語モデルをファインチューニングすることで、迎合的な傾向を抑制することを目指している。

別の研究アプローチ[263, 264]として、活性化ステアリングを用いた追従性の軽減が検討されている。この方法では、追従性と非追従性のプロンプトを組み合わせて、中間活性化の差異を平均化して得られる追従性ステアリングベクトルを生成する。推論時にこのベクトルを差し引くことで、追従性の低いLLM出力を生成することが可能となる。

**考察**: 事後学習手法による幻覚現象の軽減は、データソースや事前学習に伴う複雑さを回避できる直接的かつ効果的なアプローチである。しかし現在の研究における顕著な課題として、LLM内の能力整合性の問題に対する注目が不足している点が挙げられる。今後の研究では、能力整合性における知識の限界を理解することを優先し、幻覚現象を効果的に軽減するための能力整合性の問題に取り組む必要がある。

## 5.3. 推論関連の幻覚現象の軽減

大規模言語モデル（LLM）におけるデコーディング戦略は、生成されるコンテンツの事実性と忠実性を決定する上で極めて重要な役割を果たします。しかし、セクション3.3で分析したように、不完全なデコーディング戦略では、事実性に欠けたり、元の文脈から逸脱した出力が生成される場合があります。本節では、LLMの出力における事実性と忠実性の両方を向上させるため、デコーディング戦略を洗練させることを目的とした2つの先進的な戦略について考察します。

### 5.3.1. 事実性強化型デコーディング

事実性強化型デコーディングは、大規模言語モデルが生成する出力の事実性を高めることを目的としている。この手法は、モデルの出力を現実世界の確立された事実に厳密に整合させることで、誤った情報や誤解を招く情報の拡散リスクを最小限に抑えることを目指している。

- **事実性デコーディング**: サンプリングプロセスにおけるランダム性が、オープンエンドなテキスト生成において事実性に欠ける内容を導入し得るという認識に基づき、Leeら[160]は事実性核サンプリングアルゴリズムを提案した。この手法では、文生成プロセス全体を通じて核確率 $p$ を動的に調整する。減衰係数と下限値に基づいて核確率を動的に調整し、新たな文の開始時に核確率をリセットすることで、このデコーディング戦略は事実性コンテンツの生成と出力の多様性維持の間で最適なバランスを実現している。さらに、一部の研究[31, 220]では、大規模言語モデルの活性化空間には、事実性に関連する解釈可能な構造が存在すると仮定されている。この考え方に基づき、Liら[172]は推論時介入（ITI）手法を導入した。この手法では、まず事実的に正しい記述に関連する活性化空間内の方向を特定し、推論プロセス中にその真実性関連方向に沿って活性化を調整する。このような介入を繰り返し適用することで、大規模言語モデルはより事実性の高い応答を生成するように誘導される。同様に、Chuangら[59]は、事実性知識の保存という観点から、大規模言語モデルのデコーディングプロセスの事実性を向上させる手法を探求した。彼らは、トランスフォーマー大規模言語モデル内の事実性知識の階層的符号化を利用し、低レベル情報が初期層で、意味情報が後期層でそれぞれ捕捉されることを指摘している。[175]の研究に着想を得て、彼らはDoLaという戦略を導入した。これは、異なる層からのロジットを動的に選択・対比することで、デコーディングの事実性を精緻化する手法である。高レベル層の知識を重視し、低レベル層の知識を軽視することで、DoLaは大規模言語モデルをより事実性の高いものにし、幻覚現象を低減させる可能性を示している。

- **事後編集型のデコーディング手法**: 初期のデコーディング時に確率分布を直接修正することで幻覚現象を防ぐ従来の手法とは異なり、LLMの自己修正能力[237]を活用して、外部知識ベースに依存せずに初期生成内容を精緻化する手法である。Dhuliwalaら[74]が提案したChain-of-Verification（COVE）は、この前提に基づき、適切にプロンプトを与えればLLMが自らの誤りを修正し、より正確な事実情報を提供できるという仮定に基づいて動作する。この手法では、まず初期ドラフトを生成した後、検証用の質問を体系的に作成し、それらの質問に順次回答していくことで、最終的に精緻化された改訂版回答を生成する。同様に、Jiら[137]は医療分野に焦点を当て、反復的な自己内省プロセスを提案する。このプロセスでは、LLMが本来有する事実知識を生成する能力を活用し、提供された背景知識と一貫して整合するように回答を段階的に精緻化していく。

**議論**: 事実性解読手法は、通常各解読段階での事実性を評価するもので、大幅な性能向上をもたらす可能性があります。さらに、これらの手法はプラグアンドプレイ方式を採用しているため、計算負荷の高い訓練を必要とせずに適用可能です。ただし、これらの手法の主な制約の一つは、事実性の正確性と、生成コンテンツの多様性および情報量の維持との間で最適なバランスを取ることであり、場合によってはいずれかの側面で妥協が生じる可能性があります。一方、ポストエディット解読戦略はその有効性にもかかわらず、LLMの自己修正能力に大きく依存しており、この能力は必ずしも信頼できるものではありません。さらに、自己反省を適用することは時間を要するため、リアルタイムアプリケーションにおける実用性が制限される可能性があります。したがって、事実性と計算効率の間で最適なバランスを実現することが極めて重要です。

### 5.3.2. 忠実性強化型解読

一方、忠実性強化型解読では、提供された文脈との整合性を優先するとともに、生成コンテンツ内の一貫性を強化することにも重点を置いています。

- **文脈整合性**: この文脈において、Shiら[275]は文脈認識型復号化（CAD）を提案した。これはモデルの本来の出力分布を対比的な形式で修正する手法である[175]。CADは、文脈あり・なしの場合におけるトークン分布の差異を増幅させることで、LLMが事前知識に過度に依存するのではなく、文脈情報により重点を置くよう促す。ただし、多様性と文脈帰属の間には本質的なトレードオフが存在する[103, 363]ため、文脈情報を重視しすぎると多様性が損なわれる可能性がある。この問題に対処するため、Changら[36]は忠実性と多様性を両立させる動的復号化アルゴリズムを導入した。具体的には、文脈あり・なしの2つの復号化パスを同時に実行し、復号化過程では2つのトークン分布間のKLダイバージェンスを指針信号として用いる。この信号は、情報源の文脈がどの程度関連性があるかを示す指標として機能し、情報源が関連する場合にはトークン分布のサンプリング温度を動的に調整することで情報源帰属を改善する。並行して、Choiら[53]は知識制約型復号化（KCD）を提案した。これはトークンレベルの幻覚検出識別器を用いて文脈幻覚を特定し、トークン分布の重み付けを再調整することで忠実な生成プロセスを導くものである。さらに、出力分布を直接修正して文脈への注意力を高める手法に加え、別の研究ラインでは忠実性を向上させるための汎用的な事後編集アプローチが探究されている。Gaoら[94]は研究・修正ワークフローを採用し、研究段階ではモデルの初期応答に関する様々な側面に疑問を投げかけ、各クエリに対する証拠を収集する。一方、修正段階では、モデルの応答と証拠の間に生じる不一致を検出・修正する。同様に、Leiら[161]はまず文レベルとエンティティレベルの両方で文脈幻覚を検出し、これらの判断を統合して生成応答を精緻化する手法を採用した。さらに、いくつかの研究では、多様性と忠実な表現の発現を制約するソフトマックスボトルネックを克服するための手法が探究されている。これらのアプローチには、複数の隠れ状態を用いてソフトマックス計算を複数回実行し、結果の分布を統合する混合ソフトマックス[343]や、LLMが文脈単語をコピーすることを可能にするポインタネットワーク[37]などが含まれ、これにより文脈幻覚の発生を抑制することができる。

- **論理的一貫性**: 人間の思考プロセスに着想を得た「連鎖思考」[326]は、LLMが複雑な問題を明示的な中間ステップに分解するよう促すことで、推論プロセスの信頼性を向上させる手法として提案されている[58]。効果的ではあるものの、最近の研究[157, 302]では、LLMが生成する中間的な推論根拠がその背後にある実際の挙動を正確に反映していないことが明らかになった。特に多段階推論[61]や論理的推論[18]において、LLMが生成する中間推論根拠の一貫性を向上させるための研究分野が形成されている。Wangら[318]は、連鎖思考における自己一貫性を高めるため、知識蒸留フレームワークを採用した。まず対比的復号[175]を用いて一貫性のある推論根拠を生成した後、反事実的推論目的関数を用いて学生モデルを微調整した。これにより、根拠を考慮せずに回答を導出するような推論のショートカット[27]を効果的に排除できる。さらに、対比的復号を直接採用することで、LLMは表層的なコピーを減らし、推論ステップの欠落を防ぐことができる[229]。加えて、Liら[167]は、不誠実な推論過程における文脈、CoT、回答間の因果的関連性について詳細な分析を行った。その結果、不誠実性の問題は、CoTと回答が取得する文脈情報における不整合性に起因することが明らかになった。この問題に対処するため、彼らは推論ブリッジングという手法を提案した。これは、属性付与手法を用いて文脈情報をヒントとして想起させ、CoT推論を強化するとともに、文脈に対する意味的一貫性と属性付与スコアが低いノイズの多いCoTを除去するものである。Paulら[241]は、推論プロセスを2つのモジュールに分解した：推論モジュールは、直接的な選好最適化手法[253]は、LLMが反事実的な推論連鎖よりも正しい推論連鎖を優先するよう調整する手法であり、反事実的選好と因果関係選好の目的関数を用いて、LLMが推論過程において忠実な推論を行うよう促す推論モジュールを備えています。自然言語推論と比較して、論理的推論では厳密な論理的計算が求められるのに対し、平易なテキストでは正確な論理的構造が欠如していることが多く、これが不忠実な推論を引き起こす要因となります。この問題に対処するため、Xuら[338]はSymbolic CoT（SymbCoT）を提案しました。これは推論過程を記述するためにCoT内に記号表現を組み込む手法です。具体的には、SymbCoTは自然言語の文脈を記号表現に変換した後、論理的推論問題に対処するための段階的な計画を策定し、その後翻訳と推論連鎖を検証する検証器を用いて、忠実な論理的推論を保証します。

**考察**: 忠実性強化型デコーディング手法は、LLMの出力が提供された文脈と整合性を持つように大きく前進させ、生成コンテンツの内部一貫性を向上させます。しかしながら、文脈認識型デコーディングなどの手法は、適応メカニズムに欠けることが多く、文脈への動的な注意が求められるシナリオにおいてその有効性が制限されます。さらに、多くのデコーディング戦略では、文脈に焦点を当てない追加モデルの統合が必要となり、計算オーバーヘッドが大幅に増加し、効率が低下します。

# 6. 検索拡張生成における幻覚現象

Retrieval Augmented Generation（RAG）は、LLMの幻覚現象を軽減し、出力の事実性を向上させる有望な手法として注目されている[131, 165, 255, 277]。推論時に大規模外部知識ベースを組み込むことで、RAGはLLMに最新の知識を提供し、LLMに内在する知識の限界に起因する幻覚現象のリスクを低減する[260]。RAGはLLMの幻覚現象を軽減するために設計されているにもかかわらず、依然として幻覚現象が発生する可能性がある[16]。

RAGにおける幻覚現象は複雑な問題であり、出力が事実的に不正確であるか、誤解を招く内容となる形で現れる。このような幻覚現象は、LLMが生成した内容が現実世界の事実と一致しない場合、ユーザーのクエリを正確に反映していない場合、あるいは参照情報によって裏付けられていない場合に発生する。このような幻覚現象は主に2つの要因に起因する：**検索失敗（6.1）**と**生成ボトルネック（6.2）**。現在のRAGシステムに存在する制約要因を包括的に分析することで、検索拡張型LLMの潜在的な改善点を明らかにし、より信頼性の高い情報検索システムの実現に向けた道筋をつけることを目的としている。

## 6.1. 検索の失敗

情報検索プロセスは、RAGフレームワークにおいて極めて重要な初期段階であり、情報検索クエリに対して最も関連性の高い情報を取得する役割を担っています。このため、検索段階における失敗はRAGパイプライン全体に深刻な影響を及ぼし、幻覚現象を引き起こす可能性があります。こうした失敗は主に3つの主要な要素に起因します：ユーザークエリの形式、検索ソースの信頼性と範囲、そして検索器の有効性です。

### 6.1.1. ユーザークエリ

ユーザークエリは、RAGシステムにおける情報検索プロセスを導く上で極めて重要な役割を果たします。これらのクエリの具体性と明確性は、検索結果の有効性に決定的な影響を及ぼします。本節では、幻覚現象の要因を3つの観点――盲目的検索、曖昧なクエリの誤解釈、複雑なクエリの正確な検索における課題――から考察します。理解を深めるため、具体例を表5に示します。

**表5. LLMの幻覚現象を引き起こす可能性のあるユーザークエリの各カテゴリの具体例**
*(赤枠で囲まれた内容は幻覚的な出力結果を示しています)*

| 種類 | 具体例ダイアログ | 説明 |
| :--- | :--- | :--- |
| **検索意図** | **取得された文脈**: 高地では、水は海面レベルよりも低い温度で沸騰します。例えば95℃で沸騰します。<br>**質問**: 海面気圧下での水の沸点は何度ですか？<br>**回答**: 標準大気圧下における水の沸点は<font color="red">95℃</font>です。 | この質問は基本的な科学的事実であり、検索は不要です。しかし、システムは不必要に検索を行い、高地での沸騰に関する無関係な文脈を取得したため、誤った回答を生成しました。 |
| **曖昧なクエリ** | **取得された文脈**: ヘンネスシー・ヴェノムGTは、2014年に時速270.49マイル（435.31 km/h）の記録を達成し、世界最速の市販車となりました。<br>**質問**: トラックで最速の1マイル記録は何ですか？<br>**回答**: トラック上で最速の1マイル記録は、<font color="red">ヘンネスシー・ヴェノムGTによって32秒で達成されました。</font> | 「トラック」という言葉の曖昧さ（陸上競技のトラックか、乗り物のトラックか）により、検索システムは誤って自動車レースに関する情報を取得し、アスリートではなく車両に関する幻覚的な回答を生成しました。 |
| **複雑なクエリ** | **取得された文脈**: アレクサンダー・フレミングは、ペニシリン発見における画期的な研究で知られ、現代の抗生物質開発に大きな影響を与えました。フレミングの発見は数え切れないほどの命を救いました。<br>**質問**: ペニシリンを発見した科学者が卒業した大学はどこですか？<br>**回答**: アレクサンダー・フレミングは<font color="red">オックスフォード大学を卒業しています。</font> | 検索システムが把握できたのは、フレミングのペニシリン発見における専門的な業績に関する情報のみです。ただし、文書には彼の学歴に関する情報が含まれていないため、モデルは幻覚的な回答を生成しています。 |

- **検索意図の判定について**: すべてのクエリが検索を必要とするわけではありません。外部知識を必要としないクエリに対して盲目的に検索を行うと、かえって誤った回答を導く可能性があります。表5に示すように、「海面気圧下における水の沸点」に関するクエリは、モデルが外部検索を行わずとも回答可能な基本的な科学的事実に関するものです。しかしながら、検索システムは不適切なタイミングで活性化され、不正確な情報を盲目的に取得した結果、さらなる不適切な応答を引き起こすことがある。これを受け、複数の研究[75, 204, 228, 378]では、受動的な情報取得から能動的な情報取得への転換が提案されている。これらの戦略は一般的に、ヒューリスティックベース手法と自己認識型判断手法の2つに分類される。
- **曖昧なクエリの問題**: クエリに含まれる省略表現、共参照、曖昧性などは、検索システムが正確に関連文書を取得する能力を大きく損なう要因となります。その結果、望ましくない回答が生成される可能性が高まります。表5に示すように、「トラック上で最速のマイル走記録」というクエリの曖昧性により、検索システムは誤って自動車レースイベントに関する情報を取得してしまい、その結果モデルはアスリートではなく車両に適した回答を生成してしまいました。一般的な緩和策として、クエリの書き換え手法が用いられます。これは、クエリを精緻化し文脈から切り離すことで、関連する文書により適合したクエリを生成する手法です。
- **複雑なユーザークエリ**: 特に高度な推論を必要とするもの[286]や、複数の側面を含むもの[272, 319]は、検索システムにとって重大な課題となります。このようなクエリには高度な理解力と分解能力が求められますが、現在のキーワードベースや意味的マッチングに基づく検索手法の能力を超えることが多く、結果として部分的あるいは不正確な検索結果をもたらすことがあります。例えば表5に示すように、「ペニシリンを発見した科学者が卒業した大学はどこか」という多段階クエリの場合、直接的な検索では不完全な結果が得られることが多く、これが幻覚的な応答を引き起こす原因となります。一般的なアプローチとして、クエリ分解手法が用いられます。これは複雑なクエリをサブクエリに分解することで、より正確な情報検索を可能にする手法です。

### 6.1.2. 検索ソースの信頼性と範囲

RAGシステムの有効性を決定づける重要な要素として、検索ソースの信頼性と網羅性が挙げられます。効果的な検索は、ユーザークエリの明確さだけでなく、情報を取得するソースの品質と網羅性にも依存します。これらのソースに事実誤認や陳腐化した情報が含まれている場合、検索失敗のリスクが大幅に高まり、結果として不正確あるいは誤解を招く情報が生成される可能性があります。

RAGシステムにおける検索ソースの統合は、人工知能生成コンテンツ（AIGC）[33]の急速な進化に伴い、インターネットに流入するLLM生成コンテンツの量が増加しており、これが検索ソースとして統合されるケースが増えています[39]。この統合は、最近の実証研究[68, 339]が示すように、現代の検索モデルが人間作成コンテンツよりもLLM生成コンテンツを優先する傾向があることを示しています。Tanら[44]は、LLM生成コンテンツをRAGシステムに段階的に統合することの影響について研究を行いました。その結果、適切な介入が行われない場合、人間作成コンテンツはRAGシステム内で徐々に影響力を失っていくことが明らかになりました。さらに、Tanら[293]は、検索コンテキストにLLM生成コンテンツを組み込んだ場合のRAGシステムの性能を調査し、生成されたコンテキストが優先されるという顕著なバイアスを明らかにしました。このバイアスは、生成コンテキストと質問の間の高い類似性、および検索コンテキストの意味的不完全性に起因しています。さらに深刻な問題として、LLMが事実誤認を含む幻覚を生成する傾向が強いことが、検索ソースの信頼性問題をさらに悪化させています。LLM生成コンテンツには事実誤認が含まれることが多いため、これを検索ソースに統合すると、検索システムを誤った方向に導かれる可能性があり、結果として取得される情報の正確性と信頼性がさらに損なわれることになります。

これらのバイアスに対処するため、複数のアプローチが検討されてきた。事前学習データ処理における一般的な手法[23]に着想を得て、浅井ら[12]は、検索データストアの高品質を保証するように設計された品質フィルタを組み込んだシナリオを提案した。さらに、潘ら[238]は提案するCredibility-aware Generation（CAG）では、LLMが情報の信頼性に基づいて情報を識別・処理する能力を備えます。本手法では、情報の関連性、時間的文脈、および情報源の信頼性を考慮しながら、情報に異なる信頼性レベルを付与します。これにより、RAGシステムにおける誤った情報の影響を効果的に軽減します。

### 6.1.3. Retriever

ユーザークエリが明示的であり、検索ソースが信頼できる場合、検索プロセスの有効性はRetrieverの性能に決定的に依存します。このような状況下では、不適切なチャンク化と埋め込み手法によって、Retrieverの有効性が大きく損なわれる可能性があります。

- **チャンク化処理**: 長文文書を処理する際、文書を適切な長さのチャンクに分割する手法が広く用いられています。これは文書を段落単位（100語程度の段落）に分割する最も基本的かつ一般的な手法であり、RAGシステム[24, 109, 165]などで広く採用されています。しかし、固定長チャンク化には長文文書の構造的複雑さや依存関係を十分に捉えられないという限界があります。そこでSarthiら[267]は、文書索引化と検索システムであるRAPTORを提案しました。テキストチャンクを再帰的に埋め込み、クラスタリング、要約処理することで、RAPTORは階層構造と低レベル詳細の両方を捉えるツリー構造を構築します。検索処理において、RAPTORはLLMが様々な抽象レベルで情報を統合できるようにし、ユーザークエリに対するより包括的な文脈を提供します。固定長チャンク化とは異なり、意味的チャンク化は埋め込み類似度に基づいて文間の境界点を適応的に特定し、意味的連続性を保持します[144]。さらに、Chenら[43]は既存の検索粒度の限界を指摘しています。一方で、より長い文脈を含む粗粒度検索は理論的にはより包括的な文脈を提供できますが、LLMの注意を逸らす可能性のある余分な詳細情報も含まれがちです。他方、細粒度検索はより正確で関連性の高い情報を提供できますが、自己完結性に欠け、必要な文脈情報が不足しているという制約があります。これらの課題に対処するため、Chenら[43]は新規の検索粒度概念である命題を提案しました。これはテキスト内の原子的表現であり、それぞれが独自の事実情報を含んでおり、簡潔で自己完結型の自然言語形式で提示されます。
- **埋め込み処理**: 検索対象テキストがチャンク化された後、テキストチャンクは埋め込みモデルによってベクトル表現に変換されます。この表現方式は、ベクトルデータベース[142]として知られる著名なデータ構造によってサポートされており、データをキー-バリューペアとして体系的に整理することで効率的なテキスト検索を実現します。このようにして、類似度関数に基づいてテキスト表現とクエリ表現の類似性を計算することで、関連性スコアを求めることができます。ただし、最適でない埋め込みモデルを使用すると、パフォーマンスが低下し、チャンクとユーザークエリのマッチング精度や類似性に影響を及ぼし、LLMを誤誘導する可能性があります。通常、標準的な埋め込みモデル[96, 130, 147, 382]は、エンコーダベースのアーキテクチャ（例えばBERT[73]、RoBERTa[191]）を用いてクエリとテキスト表現を学習しますが、これは対照学習[304]によって構築され、正例のクエリ-文書ペアとランダムな負例ペアの集合を対比させることで損失関数が構築されます。ただし、これらの埋め込み表現は、医療や金融アプリケーション[222, 295]などの新規ドメインに適用した場合、限界が露呈することがあります。このような場合、最近の研究[71, 234, 277, 332]では、ドメイン固有データを用いて埋め込みモデルをファインチューニングすることで、検索関連性を向上させることが提案されています。例えば、REPLUG[277]では、回答の言語モデリングスコアを高密度リトリエバの訓練用プロキシ信号として利用しています。

近年、Muennighoffら[221]が「生成的表現型指示チューニング」という新たな手法を提案している。これは、単一のLLMを生成タスクと埋め込みタスクの両方に対応できるように訓練する手法であり、RAGシステムにおける推論遅延を大幅に低減するため、表現をキャッシュする点が特徴である。こうした進展がある一方で、この分野には依然として課題が残されており、特にOpenAIのtext-embedding-ada-002など、高性能でありながらアクセスが困難な埋め込みモデルのファインチューニングに関しては未解決の問題が存在する。このギャップを埋めるため、Zhangら[367]は、訓練可能な埋め込みモデルを追加することでブラックボックス型埋め込みモデルのファインチューニングを行う新たな手法を提案しており、これによりブラックボックス型埋め込みモデルの性能を大幅に向上させることが可能となっている。

## 6.2. 生成処理におけるボトルネック

検索処理に続く生成段階は、検索された情報を正確に反映したコンテンツを生成する重要な工程である。しかし、この段階には重大なボトルネックが存在し、これが幻覚（hallucinations）を引き起こす要因となる場合がある。本節では、これらのボトルネックと密接に関連するLLMの2つの重要な機能について概説する：**文脈理解**と**文脈整合性**である。これらはいずれも、RAGシステムの信頼性と妥当性を確保する上で重要な役割を果たす。

### 6.2.1. 文脈理解能力

文脈理解とは、検索プロセスで取得された文脈情報を的確に理解し活用する能力を指します。本節では、LLMが文脈理解能力を維持する上で影響を与える主要な要因について論じます。これらの要因は大きく3つのカテゴリに分類できます：
1. 文脈内におけるノイズの多い検索結果の存在
2. 文脈間の競合
3. 文脈情報の活用不足

- **ノイズの多い文脈**: セクション6.1で強調したように、検索プロセスにおける失敗は不可避的に無関係な情報を導入し、これが生成段階に伝播します。生成器がこのような無関係な検索結果に対して十分な頑健性を備えていない場合、生成器を誤誘導し、幻覚現象を引き起こす可能性があります[65]。Yoranら[352]は、現在の検索拡張型LLMの頑健性について包括的な分析を行い、ランダムな検索手法では性能が大幅に低下することを明らかにした。NLIモデルを用いて無関係な文章をフィルタリングする方法は有効であるものの、この手法では意図せず有用な文章まで排除してしまうというトレードオフが存在する。より効果的な解決策として、LLMが訓練データに無関係な文脈を組み込むことで、無関係な文脈を無視するように学習させる方法が提案されている。同様に、Yuら[357]はChain-of-Noteという手法を導入し、LLMが最初に検索された文脈に対する読解メモを生成し、その後最終的な回答を策定する手法を提案した。これにより、LLMは無関係な検索結果をフィルタリングしてノイズ耐性を向上させるだけでなく、検索結果が不十分でユーザーの質問に答えられない場合には"不明"と回答することが可能となる。文脈内の無関係な内容を無視するように学習することに加え、いくつかの研究[139, 176, 323, 336]では、文脈を圧縮して無関係な情報をフィルタリングする手法が提案されている。
- **文脈競合問題**: 検索拡張型LLMはパラメトリック知識と文脈知識の統合効果によって回答を生成する。Longpreら[194]がオープンドメイン質問応答における文脈競合問題を初めて調査した際、回答のすべてのスパンを検索文脈内の置換エンティティで置き換えることで、競合が自動的に生成される状況を設定した。調査結果によると、生成型QAリーダーモデル（T5など）は文脈情報よりもパラメトリック記憶を優先的に信頼する傾向がある。さらに、検索モデルを訓練して文脈証拠を信頼するよう学習させ、置換エンティティによる拡張訓練例を用いることで、パラメトリック知識への過度の依存という問題が緩和される。同様の知見はLiら[166]によっても報告されており、反事実的文脈でLLMを微調整することで、矛盾する文脈を扱う際のLLMの制御可能性を効果的に向上させられることが示されている。
- **文脈活用の課題**: 大規模言語モデル（LLM）は事実確認型質問に対する適切な回答を取得できるものの、特に長文文脈ウィンドウの中間部分に位置する情報に関しては、性能低下が顕著に現れるという問題が指摘されている（いわゆる「中間部分消失」現象[189]）。この問題は事実確認型質問応答にとどまらず、抽象要約[257]、長文質問応答[41]、パッセージランキング[294]といった分野でも確認されている。この現象の一因として、オープンソースLLMで広く採用されている回転位置埋め込み（RoPE）[287]の使用が考えられる。RoPEは長文外挿性能[380]において優れた性能を発揮するためである。代表的な相対位置埋め込み手法であるRoPEは、長期的な減衰特性を備えており、これが本質的にLLMに現在または近接するトークンを優先させるバイアスを生じさせ、結果としてより遠方のトークンに対する注意力を低下させる。もう一つの要因として、最も顕著な情報が事前学習データの先頭または末尾に集中する傾向があり、これはニュースレポート[257]などで一般的に観察される特性である。このような問題は、検索拡張型LLMにおいて課題を引き起こす。検索拡張型LLMは通常、より多くの検索文書を収容できるように長文設計がなされているためである。

### 6.2.2. 文脈整合性の確保

文脈整合性とは、LLMの出力が関連する文脈に対して忠実に整合していることを保証するものです。本節では、文脈整合性を実現するための主要な要素を以下のように説明します：
1. 出典の明示
2. 忠実なデコーディング処理

- **情報源の明示化**: 検索拡張型LLMにおいて、情報源の明示化とは、モデルが生成プロセスにおいて参照した情報源を特定・活用するプロセスを指します。この機能は、RAGシステムが生成する出力が関連性を持つだけでなく、信頼できる情報源に基づいていることを保証する上で極めて重要です。RAGシステムにおける情報源の明示化を実現するために、近年の研究では3つの主要なアプローチが提案されています。
    1. **計画→生成アプローチ**: Fierroら[88]は情報源明示化のためのブループリントモデルを導入しました。このモデルでは、テキスト生成プロセスの青写真として機能する一連の質問を生成プロセスの内容と順序を規定するブループリントとして概念化しています。
    2. **生成→内省アプローチ**: Asaiら[11]は、内省トークンを備えたテキスト生成を行うようにLLMを訓練する手法を提案しました。この内省トークンにより、LLMは情報源の取得、その関連性の評価、および生成プロセス自体の批評を通じて情報源明示化を保証する能力を獲得します。
    3. **自己内省メカニズム**: 情報源明示化のために外部の教師あり信号を活用するだけでなく、Qiら[248]はモデル内部の信号を利用する自己内省メカニズムを提案しました。

- **忠実なデコーディング生成**: RAGパイプラインにおいて高度に関連性の高いコンテンツをモデルのコンテキストに組み込むための大幅な最適化が行われているにもかかわらず、現在の大規模言語モデル（LLM）は依然として忠実な生成を保証できません。LLMが内部知識と文脈情報が矛盾する場合に関連文脈を不適切に利用する現象は、LLMの内部事前分布と外部証拠の間の綱引き状態を生み出します。この問題に対処するため、最近の研究[275, 330]ではRAGシステムにおける忠実なデコーディングに焦点を当て、モデルが文脈情報に忠実に沿ったコンテンツを生成する能力の向上を図っています。Shiら[275]は文脈認識型デコーディングを提案し、モデルの元の出力確率分布を相互情報量（PMI）の形式に変換します。この手法は、文脈あり／なしの場合における出力確率の差異を増幅させることで、LLMが提供された文脈に対して忠実である能力を高めます。

# 7. 今後の議論

LLMにおける幻覚現象に関する研究分野が進化を続ける中、私たちは次なる研究のフロンティアへと焦点を移しています。具体的には、視覚言語モデルにおける幻覚現象の研究領域（7.1）や、LLM内における知識の境界を明確にし理解することの課題について考察します（7.2）。

## 7.1. 大規模視覚言語モデルにおけるマルチモーダル幻覚現象

大規模視覚言語事前学習データセットから限定的な視覚言語能力を獲得する従来のマルチモーダル事前学習モデル[170, 197, 325, 387]とは異なり、大規模視覚言語モデル（LVLM）は先進的な大規模言語モデルを活用することで、人間や環境とのインタラクション能力を最大限に引き出すことを目指している。LVLMの多様な応用可能性は、同時にこれらのシステムの信頼性維持に新たな課題をもたらしている。最近の研究では、現在のLVLMがマルチモーダル幻覚現象に悩まされていることが明らかになっている。これは、モデルが対応する視覚情報と整合しない応答を生成する現象である[104, 187, 297]。このようなマルチモーダル幻覚現象は、LVLMを実世界のシナリオに適用する際に予期せぬ挙動を引き起こす可能性があるため、さらなる調査と緩和策が必要である。

Liら[178]およびLoveniaら[195]は、LVLMにおける物体幻覚現象の評価に向けた第一歩を踏み出した。評価実験の結果、現在のLVLMは関連する画像に対して一貫性のない応答を生成する傾向があり、具体的には存在しない物体、誤った物体タイプや属性、意味的関係性の誤りなどが確認されている[315, 361]。さらに、Liuら[185]、Zongら[395]、Liuら[184]の研究では、LVLMが言語的な事前知識への過度の依存や、不適切なユーザー入力に対する防御能力の低さから、容易に誤誘導され性能が大幅に低下することが示されている[112, 134]。Jiangら[138]、Wangら[315]、Jingら[141]の研究では、マルチモーダル幻覚現象を包括的に評価する方向へと一歩前進した。さらに、複数の画像が提示された場合、LVLMは視覚的文脈の一部を混同したり見落としたりするだけでなく、それらの間の時間的・論理的関係性を理解することも困難であり、これが多くのシナリオでの利用を妨げる可能性がある。しかし、このような障害の原因を適切に特定し対処するためには、さらなる研究が必要である。知覚エラーが確認されているにもかかわらず、LVLMはすべての視覚要素を正しく認識している場合でも、誤った論理的推論結果を生成することがあり、この点についてはさらなる調査が必要である。

より堅牢な大規模視覚言語モデルの構築に向けた取り組みが進められている。Gunjalら[108]、Luら[196]、Wangら[316]、Liuら[185]は、モデルをさらに微調整することでより真実性が高く有益な応答を生成する手法を提案した。別の研究グループは、生成された矛盾する内容を事後的に修正する手法を採用しており、例えば[391]や[349]では専門家モデルを導入している。外部ツールへの依存を解消するため、Lengら[162]、Huangら[122]、Zhaoら[379]は、LVLM自体の機能を最大限に活用し、幻覚現象を軽減する手法を試みた。これらの手法は効果的であることが証明されているものの、通常、追加のデータアノテーション、視覚専門家の関与、訓練フェーズ、計算コストが必要となるため、LVLMが様々な分野に効果的に拡張・一般化することを妨げている。したがって、より普遍的なアプローチとして、LVLMの信頼性をさらに高めるために、より忠実で大規模な視覚-テキスト事前学習およびアライメント手法など、より普遍的なアプローチが期待される。

## 7.2. LLMにおける知識境界の理解

大規模データから事実に基づく知識を抽出する能力において顕著な成果を上げている一方で、LLMは自らの知識範囲を正確に認識することに依然として課題を抱えています。この限界が、LLMが自らの知識範囲を認識しないまま誤った情報を確信を持って生成する「幻覚現象」を引き起こす要因となっています [235, 261, 384]。多くの研究が、LLMの知識範囲の限界を探るために、多肢選択問題における正解の出現確率を評価する手法 [143] や、意味が不確かな文集合におけるモデルの出力不確実性を文の類似性評価によって定量化する手法など、様々なアプローチを採用しています。

さらに、研究分野 [13, 31, 172, 220] では、LLMの活性化空間に真実性に関する潜在的な構造が存在することが明らかになっている。最近の研究 [281] では、LLMが回答不能な質問に対する不回答性を符号化する能力について、かなりの証拠が示されている。ただし、これらのモデルは過信傾向を示し、回答不能な質問に対して幻覚を生成することが知られている。それにもかかわらず、Levinstein と Herrmann [163] は、経験的および概念的な手法を用いて、LLMが信念を持っているかどうかを検証した。彼らの実証結果から、現在のLLM向け嘘発見器手法はまだ完全に信頼できるものではなく、Burns ら [31] および Azaria と Mitchell [13] が提案した検証手法も十分に一般化できていないことが示唆される。したがって、LLMの内部的な信念を効果的に検証できるかどうかについては、現在も研究が続けられており、さらなる調査が必要である。

# 8. 結論

本包括的調査では、大規模言語モデルにおける幻覚現象について詳細に検証し、その根本原因、先駆的な検出手法、関連するベンチマーク指標、および効果的な軽減策について体系的に考察しています。一定の進展は見られるものの、LLMにおける幻覚現象の問題は依然として重要かつ継続的な課題であり、継続的な研究が求められています。さらに、本調査は、堅牢な情報検索システムと信頼性の高い人工知能の発展に取り組む研究者にとって、指針となる重要な知見を提供することを目的としています。幻覚現象という複雑な課題に取り組むことで、AI技術の進化をより信頼性が高く安全な方向へと導くための貴重な知見を、これらの研究者に提供したいと考えています。

---
## 参考文献
*(参考文献リストは省略)*
